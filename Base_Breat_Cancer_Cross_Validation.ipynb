{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEtPQgAUHZBWgceQmt7ohs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielddrs/breast-cancer/blob/main/Base_Breat_Cancer_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Breast Cancer - Classificação binária com Validação cruzada"
      ],
      "metadata": {
        "id": "DFdF8d75dK32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 1 - Importação das bibliotecas"
      ],
      "metadata": {
        "id": "BK1VI-IudQg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU scikit-learn"
      ],
      "metadata": {
        "id": "fcIuGnJxKCh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Biblioteca para realizar a validação cruzada\n",
        "!pip install -q skorch"
      ],
      "metadata": {
        "id": "eIoqklsZdgMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6851128a-6774-483b-aa0f-83b41953a869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/239.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skorch import NeuralNetBinaryClassifier"
      ],
      "metadata": {
        "id": "gqXiDggGd9Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yNPaYUyiKSBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/cursoDeepLearning/Deep Learing de A à Z com PyTorch.zip' -d '/content/cursoDeepLearning'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRofcoHYG24b",
        "outputId": "7c84f1b4-d5c7-46aa-c91a-4f5003d5728d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/cursoDeepLearning/Deep Learing de A à Z com PyTorch.zip\n",
            "   creating: /content/cursoDeepLearning/Bases/\n",
            "  inflating: /content/cursoDeepLearning/Bases/petr4_teste.csv  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._petr4_teste.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/.DS_Store  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._.DS_Store  \n",
            "  inflating: /content/cursoDeepLearning/Bases/poluicao.csv  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._poluicao.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/personagens.csv  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._personagens.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/autos.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/iris.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/games.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/petr4_treinamento.csv  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._petr4_treinamento.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/entradas_breast.csv  \n",
            "  inflating: /content/cursoDeepLearning/Bases/dataset.zip  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Bases/._dataset.zip  \n",
            "  inflating: /content/cursoDeepLearning/Bases/dataset_personagens.zip  \n",
            "  inflating: /content/cursoDeepLearning/Bases/saidas_breast.csv  \n",
            "   creating: /content/cursoDeepLearning/Slides/\n",
            "  inflating: /content/cursoDeepLearning/Slides/Boas-vindas e conteúdo.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Boas-vindas e conteúdo.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/.DS_Store  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._.DS_Store  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Outros slides.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Outros slides.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Formação em Deep Learning com PyTorch e Python.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Formação em Deep Learning com PyTorch e Python.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Boltzmann machines.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Boltzmann machines.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Redes neurais recorrentes.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Redes neurais recorrentes.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Redes neurais artificiais.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Redes neurais artificiais.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Mapas auto organizáveis.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Mapas auto organizáveis.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Style Transfer.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Style Transfer.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Generative adversarial networks.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Generative adversarial networks.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Redes neurais convolucionais.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Redes neurais convolucionais.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Autoencoders.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Autoencoders.pdf  \n",
            "  inflating: /content/cursoDeepLearning/Slides/Transfer Learning.pdf  \n",
            "  inflating: /content/cursoDeepLearning/__MACOSX/Slides/._Transfer Learning.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 2 - Base de Dados"
      ],
      "metadata": {
        "id": "lgFxdAeeelCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg55hU1qvJwr",
        "outputId": "1cd7860e-fcf8-4dce-f661-d326ee9c6900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5cfcfbd610>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = pd.read_csv('/content/cursoDeepLearning/Bases/entradas_breast.csv')\n",
        "classes = pd.read_csv('/content/cursoDeepLearning/Bases/saidas_breast.csv')"
      ],
      "metadata": {
        "id": "7hA_nrmO3SgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=classes['0']);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "D4-t_Ea83cdL",
        "outputId": "559cd415-bd18-4aa5-dc10-a1a7c2e9692c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwwijzL8OiNo",
        "outputId": "5c42319b-adc1-426e-be73-c0ae7176c84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = np.array(previsores, dtype='float32')\n",
        "classes = np.array(classes, dtype='float32').squeeze(1)"
      ],
      "metadata": {
        "id": "PHWxHD2iOo5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIatevskPLkN",
        "outputId": "0ae25057-8005-461e-d595-dbd782a254dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 3 - Classe para estrutura da rede neural"
      ],
      "metadata": {
        "id": "f01OmdQuenSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O Skorch exige que a RN seja passada por uma classe que herda da classe module do pytorch\n",
        "class classificador_torch(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # 30 -> 16 -> 16 -> 1\n",
        "    # Primeira camada densa\n",
        "    self.dense0 = nn.Linear(30, 16)\n",
        "    # Definindo a inicialização dos pesos\n",
        "    torch.nn.init.uniform_(self.dense0.weight)\n",
        "    # Definindo a função de ativação\n",
        "    self.activation0 = nn.ReLU()\n",
        "    # Com isso, já criamos a ligação entrada com a primeira hidden layer\n",
        "\n",
        "    # Segunda camada densa\n",
        "    self.dense1 = nn.Linear(16, 16)\n",
        "    torch.nn.init.uniform_(self.dense1.weight)\n",
        "    self.activation1 = nn.ReLU()\n",
        "\n",
        "    # Terceira camada densa\n",
        "    self.dense2 = nn.Linear(16, 1)\n",
        "    torch.nn.init.uniform_(self.dense2.weight)\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X = Inputs que vamos receber\n",
        "\n",
        "    #Realizando a ligação entre as camadas\n",
        "    X = self.dense0(X)\n",
        "    X = self.activation0(X)\n",
        "\n",
        "    X = self.dense1(X)\n",
        "    X = self.activation1(X)\n",
        "\n",
        "    X = self.dense2(X)\n",
        "    X = self.output(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "68kGqp-VPs8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 4 - Skorch"
      ],
      "metadata": {
        "id": "_EZL4j2lPeS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
        "                                                  criterion=torch.nn.BCELoss,\n",
        "                                                  optimizer=torch.optim.Adam,\n",
        "                                                  lr=0.001,\n",
        "                                                  optimizer__weight_decay=0.0001,\n",
        "                                                  max_epochs=100,\n",
        "                                                  batch_size=10,\n",
        "                                                  train_split=False)\n",
        "# Neste caso, usamos o False no split pois faremos a separação na validação cruzada"
      ],
      "metadata": {
        "id": "Ka5P-Ax4CldK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceba que quando usamos o Skorch, precisamos criar dois classificadores: Torch e Skorch. Só assim ele será compatível com o Sklearn"
      ],
      "metadata": {
        "id": "hXtvwsKmD789"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 5 - Validação Cruzada"
      ],
      "metadata": {
        "id": "wBQ29AUqPjsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = cross_val_score(classificador_sklearn, previsores, classes, cv = 10, scoring = 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4zaIAPFEVQ8",
        "outputId": "e4f24aa4-8756-4a5d-efb0-983cb72ff974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.0870\n",
            "      2       37.1094  0.0733\n",
            "      3       37.1094  0.0716\n",
            "      4       37.1094  0.0730\n",
            "      5       37.1094  0.0707\n",
            "      6       37.1094  0.0738\n",
            "      7       37.1094  0.0830\n",
            "      8       37.1094  0.0812\n",
            "      9       37.1094  0.0702\n",
            "     10       37.1094  0.0798\n",
            "     11       37.1094  0.0727\n",
            "     12       37.1094  0.0724\n",
            "     13       37.1094  0.0749\n",
            "     14       37.1094  0.0724\n",
            "     15       37.1094  0.0738\n",
            "     16       37.1094  0.0730\n",
            "     17       37.1094  0.0724\n",
            "     18       37.1094  0.0675\n",
            "     19       37.1094  0.0744\n",
            "     20       37.1094  0.0782\n",
            "     21       37.1094  0.0693\n",
            "     22       37.1094  0.0749\n",
            "     23       37.1094  0.0703\n",
            "     24       37.1094  0.0749\n",
            "     25       37.1094  0.0689\n",
            "     26       37.1094  0.0737\n",
            "     27       37.1094  0.0741\n",
            "     28       37.1094  0.0720\n",
            "     29       37.1094  0.0712\n",
            "     30       37.1094  0.0743\n",
            "     31       37.1094  0.0702\n",
            "     32       37.1094  0.0782\n",
            "     33       37.1094  0.0750\n",
            "     34       37.1094  0.0859\n",
            "     35       37.1094  0.0693\n",
            "     36       37.1094  0.0693\n",
            "     37       37.1094  0.0692\n",
            "     38       37.1094  0.0700\n",
            "     39       37.1094  0.0752\n",
            "     40       37.1094  0.0772\n",
            "     41       37.1094  0.0767\n",
            "     42       37.1094  0.0709\n",
            "     43       37.1094  0.0760\n",
            "     44       37.1094  0.0723\n",
            "     45       37.1094  0.0709\n",
            "     46       37.1094  0.0738\n",
            "     47       37.1094  0.0803\n",
            "     48       37.1094  0.0919\n",
            "     49       37.1094  0.0791\n",
            "     50       37.1094  0.0822\n",
            "     51       37.1094  0.0745\n",
            "     52       37.1094  0.0823\n",
            "     53       37.1094  0.0754\n",
            "     54       37.1094  0.0755\n",
            "     55       37.1094  0.0756\n",
            "     56       37.1094  0.0787\n",
            "     57       37.1094  0.0745\n",
            "     58       37.1094  0.0747\n",
            "     59       37.1094  0.0781\n",
            "     60       37.1094  0.0878\n",
            "     61       37.1094  0.0749\n",
            "     62       37.1094  0.0727\n",
            "     63       37.1094  0.0794\n",
            "     64       37.1094  0.0817\n",
            "     65       37.1094  0.0762\n",
            "     66       37.1094  0.0741\n",
            "     67       37.1094  0.0875\n",
            "     68       37.1094  0.0859\n",
            "     69       37.1094  0.0728\n",
            "     70       37.1094  0.0761\n",
            "     71       37.1094  0.0802\n",
            "     72       37.1094  0.0751\n",
            "     73       37.1094  0.0831\n",
            "     74       37.1094  0.0735\n",
            "     75       37.1094  0.0740\n",
            "     76       37.1094  0.0711\n",
            "     77       37.1094  0.0746\n",
            "     78       37.1094  0.0719\n",
            "     79       37.1094  0.0732\n",
            "     80       37.1094  0.0726\n",
            "     81       37.1094  0.0729\n",
            "     82       37.1094  0.0778\n",
            "     83       37.1094  0.0721\n",
            "     84       37.1094  0.0735\n",
            "     85       37.1094  0.0710\n",
            "     86       37.1094  0.0700\n",
            "     87       37.1094  0.0918\n",
            "     88       37.1094  0.0897\n",
            "     89       \u001b[36m11.1575\u001b[0m  0.0767\n",
            "     90        \u001b[36m0.6812\u001b[0m  0.0823\n",
            "     91        \u001b[36m0.6752\u001b[0m  0.1062\n",
            "     92        \u001b[36m0.6708\u001b[0m  0.1020\n",
            "     93        \u001b[36m0.6676\u001b[0m  0.1021\n",
            "     94        \u001b[36m0.6654\u001b[0m  0.0968\n",
            "     95        \u001b[36m0.6640\u001b[0m  0.0934\n",
            "     96        \u001b[36m0.6629\u001b[0m  0.0967\n",
            "     97        \u001b[36m0.6622\u001b[0m  0.1067\n",
            "     98        \u001b[36m0.6617\u001b[0m  0.1030\n",
            "     99        \u001b[36m0.6613\u001b[0m  0.0968\n",
            "    100        \u001b[36m0.6611\u001b[0m  0.1004\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.0943\n",
            "      2       37.1094  0.0944\n",
            "      3       37.1094  0.0940\n",
            "      4       37.1094  0.1003\n",
            "      5       37.1094  0.1107\n",
            "      6       37.1094  0.0985\n",
            "      7       37.1094  0.1069\n",
            "      8       37.1094  0.1020\n",
            "      9       37.1094  0.0993\n",
            "     10       37.1094  0.0925\n",
            "     11       37.1094  0.0910\n",
            "     12       37.1094  0.0945\n",
            "     13       37.1094  0.0914\n",
            "     14       37.1094  0.0995\n",
            "     15       37.1094  0.0923\n",
            "     16       37.1094  0.0900\n",
            "     17       37.1094  0.1001\n",
            "     18       37.1094  0.1093\n",
            "     19       37.1094  0.0913\n",
            "     20       37.1094  0.0946\n",
            "     21       37.1094  0.0944\n",
            "     22       37.1094  0.0956\n",
            "     23       37.1094  0.0908\n",
            "     24       37.1094  0.1023\n",
            "     25       37.1094  0.1173\n",
            "     26       37.1094  0.1026\n",
            "     27       37.1094  0.1065\n",
            "     28       37.1094  0.1071\n",
            "     29       37.1094  0.1200\n",
            "     30       \u001b[36m12.6598\u001b[0m  0.1143\n",
            "     31        \u001b[36m0.6355\u001b[0m  0.1105\n",
            "     32        \u001b[36m0.6324\u001b[0m  0.1083\n",
            "     33        \u001b[36m0.6305\u001b[0m  0.1077\n",
            "     34        \u001b[36m0.6291\u001b[0m  0.1099\n",
            "     35        \u001b[36m0.6281\u001b[0m  0.1013\n",
            "     36        \u001b[36m0.6273\u001b[0m  0.0741\n",
            "     37        \u001b[36m0.6267\u001b[0m  0.0830\n",
            "     38        \u001b[36m0.6261\u001b[0m  0.0739\n",
            "     39        \u001b[36m0.6257\u001b[0m  0.0774\n",
            "     40        \u001b[36m0.6253\u001b[0m  0.0743\n",
            "     41        \u001b[36m0.6250\u001b[0m  0.0733\n",
            "     42        \u001b[36m0.6248\u001b[0m  0.0730\n",
            "     43        \u001b[36m0.6245\u001b[0m  0.0708\n",
            "     44        \u001b[36m0.6243\u001b[0m  0.0759\n",
            "     45        \u001b[36m0.6242\u001b[0m  0.0728\n",
            "     46        \u001b[36m0.6240\u001b[0m  0.0714\n",
            "     47        \u001b[36m0.6239\u001b[0m  0.0706\n",
            "     48        \u001b[36m0.6238\u001b[0m  0.0774\n",
            "     49        \u001b[36m0.6237\u001b[0m  0.0731\n",
            "     50        \u001b[36m0.6236\u001b[0m  0.0715\n",
            "     51        \u001b[36m0.6235\u001b[0m  0.0764\n",
            "     52        \u001b[36m0.6234\u001b[0m  0.0705\n",
            "     53        \u001b[36m0.6234\u001b[0m  0.0761\n",
            "     54        \u001b[36m0.6233\u001b[0m  0.0754\n",
            "     55        \u001b[36m0.6233\u001b[0m  0.0717\n",
            "     56        \u001b[36m0.6232\u001b[0m  0.0727\n",
            "     57        \u001b[36m0.6232\u001b[0m  0.0783\n",
            "     58        \u001b[36m0.6231\u001b[0m  0.0733\n",
            "     59        \u001b[36m0.6231\u001b[0m  0.0714\n",
            "     60        \u001b[36m0.6231\u001b[0m  0.0740\n",
            "     61        \u001b[36m0.6230\u001b[0m  0.0775\n",
            "     62        \u001b[36m0.6230\u001b[0m  0.0741\n",
            "     63        \u001b[36m0.6230\u001b[0m  0.0727\n",
            "     64        \u001b[36m0.6230\u001b[0m  0.0776\n",
            "     65        \u001b[36m0.6230\u001b[0m  0.0725\n",
            "     66        \u001b[36m0.6229\u001b[0m  0.0748\n",
            "     67        \u001b[36m0.6229\u001b[0m  0.0736\n",
            "     68        \u001b[36m0.6229\u001b[0m  0.0724\n",
            "     69        \u001b[36m0.6229\u001b[0m  0.0722\n",
            "     70        \u001b[36m0.6229\u001b[0m  0.0716\n",
            "     71        \u001b[36m0.6229\u001b[0m  0.0750\n",
            "     72        \u001b[36m0.6229\u001b[0m  0.0762\n",
            "     73        \u001b[36m0.6229\u001b[0m  0.0716\n",
            "     74        \u001b[36m0.6229\u001b[0m  0.0720\n",
            "     75        \u001b[36m0.6228\u001b[0m  0.0798\n",
            "     76        \u001b[36m0.6228\u001b[0m  0.0731\n",
            "     77        \u001b[36m0.6228\u001b[0m  0.0701\n",
            "     78        \u001b[36m0.6228\u001b[0m  0.0783\n",
            "     79        \u001b[36m0.6228\u001b[0m  0.0720\n",
            "     80        \u001b[36m0.6228\u001b[0m  0.0741\n",
            "     81        \u001b[36m0.6228\u001b[0m  0.0731\n",
            "     82        \u001b[36m0.6228\u001b[0m  0.0691\n",
            "     83        \u001b[36m0.6228\u001b[0m  0.0696\n",
            "     84        \u001b[36m0.6228\u001b[0m  0.0775\n",
            "     85        \u001b[36m0.6228\u001b[0m  0.0697\n",
            "     86        \u001b[36m0.6228\u001b[0m  0.0703\n",
            "     87        \u001b[36m0.6228\u001b[0m  0.0732\n",
            "     88        \u001b[36m0.6228\u001b[0m  0.0723\n",
            "     89        \u001b[36m0.6228\u001b[0m  0.0750\n",
            "     90        \u001b[36m0.6228\u001b[0m  0.0728\n",
            "     91        \u001b[36m0.6228\u001b[0m  0.0732\n",
            "     92        \u001b[36m0.6228\u001b[0m  0.0732\n",
            "     93        \u001b[36m0.6228\u001b[0m  0.0772\n",
            "     94        \u001b[36m0.6228\u001b[0m  0.0738\n",
            "     95        \u001b[36m0.6228\u001b[0m  0.0706\n",
            "     96        \u001b[36m0.6228\u001b[0m  0.0689\n",
            "     97        \u001b[36m0.6228\u001b[0m  0.0751\n",
            "     98        \u001b[36m0.6228\u001b[0m  0.0719\n",
            "     99        \u001b[36m0.6228\u001b[0m  0.0728\n",
            "    100        \u001b[36m0.6228\u001b[0m  0.0730\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0713\n",
            "      2       37.3047  0.0755\n",
            "      3       37.3047  0.0748\n",
            "      4       37.3047  0.0722\n",
            "      5       37.3047  0.0775\n",
            "      6       37.3047  0.0718\n",
            "      7       37.3047  0.0695\n",
            "      8       37.3047  0.0711\n",
            "      9       37.3047  0.0701\n",
            "     10       37.3047  0.0714\n",
            "     11       37.3047  0.0708\n",
            "     12       37.3047  0.0755\n",
            "     13       37.3047  0.0727\n",
            "     14       37.3047  0.0763\n",
            "     15       37.3047  0.0801\n",
            "     16       37.3047  0.0735\n",
            "     17       37.3047  0.0701\n",
            "     18       37.3047  0.0731\n",
            "     19       37.3047  0.0845\n",
            "     20       37.3047  0.0720\n",
            "     21       37.3047  0.0725\n",
            "     22       37.3047  0.0750\n",
            "     23       37.3047  0.0893\n",
            "     24       37.3047  0.0797\n",
            "     25       \u001b[36m14.9760\u001b[0m  0.0766\n",
            "     26        \u001b[36m0.5576\u001b[0m  0.0716\n",
            "     27        \u001b[36m0.5261\u001b[0m  0.0716\n",
            "     28        \u001b[36m0.5089\u001b[0m  0.0733\n",
            "     29        \u001b[36m0.4956\u001b[0m  0.0739\n",
            "     30        \u001b[36m0.4878\u001b[0m  0.0735\n",
            "     31        \u001b[36m0.4818\u001b[0m  0.0760\n",
            "     32        \u001b[36m0.4765\u001b[0m  0.0864\n",
            "     33        \u001b[36m0.4709\u001b[0m  0.0807\n",
            "     34        \u001b[36m0.4641\u001b[0m  0.0729\n",
            "     35        \u001b[36m0.4585\u001b[0m  0.0761\n",
            "     36        \u001b[36m0.4502\u001b[0m  0.0726\n",
            "     37        \u001b[36m0.4365\u001b[0m  0.0807\n",
            "     38        \u001b[36m0.4267\u001b[0m  0.0760\n",
            "     39        \u001b[36m0.4165\u001b[0m  0.0732\n",
            "     40        \u001b[36m0.4072\u001b[0m  0.0709\n",
            "     41        \u001b[36m0.3994\u001b[0m  0.0745\n",
            "     42        \u001b[36m0.3927\u001b[0m  0.0800\n",
            "     43        \u001b[36m0.3849\u001b[0m  0.0764\n",
            "     44        \u001b[36m0.3780\u001b[0m  0.0779\n",
            "     45        \u001b[36m0.3717\u001b[0m  0.0855\n",
            "     46        \u001b[36m0.3652\u001b[0m  0.0748\n",
            "     47        \u001b[36m0.3607\u001b[0m  0.0704\n",
            "     48        \u001b[36m0.3561\u001b[0m  0.0735\n",
            "     49        \u001b[36m0.3529\u001b[0m  0.0706\n",
            "     50        \u001b[36m0.3462\u001b[0m  0.0753\n",
            "     51        \u001b[36m0.3416\u001b[0m  0.0730\n",
            "     52        \u001b[36m0.3377\u001b[0m  0.0716\n",
            "     53        \u001b[36m0.3330\u001b[0m  0.0701\n",
            "     54        \u001b[36m0.3287\u001b[0m  0.0702\n",
            "     55        \u001b[36m0.3236\u001b[0m  0.0711\n",
            "     56        \u001b[36m0.3193\u001b[0m  0.0736\n",
            "     57        0.3204  0.0720\n",
            "     58        \u001b[36m0.3149\u001b[0m  0.0744\n",
            "     59        \u001b[36m0.3104\u001b[0m  0.0805\n",
            "     60        \u001b[36m0.3075\u001b[0m  0.0724\n",
            "     61        0.3079  0.0754\n",
            "     62        \u001b[36m0.3053\u001b[0m  0.0709\n",
            "     63        \u001b[36m0.3021\u001b[0m  0.0747\n",
            "     64        \u001b[36m0.2987\u001b[0m  0.0713\n",
            "     65        \u001b[36m0.2970\u001b[0m  0.0747\n",
            "     66        \u001b[36m0.2953\u001b[0m  0.0718\n",
            "     67        \u001b[36m0.2941\u001b[0m  0.0804\n",
            "     68        \u001b[36m0.2912\u001b[0m  0.1024\n",
            "     69        \u001b[36m0.2853\u001b[0m  0.0969\n",
            "     70        \u001b[36m0.2829\u001b[0m  0.0987\n",
            "     71        \u001b[36m0.2818\u001b[0m  0.0997\n",
            "     72        \u001b[36m0.2790\u001b[0m  0.0989\n",
            "     73        \u001b[36m0.2772\u001b[0m  0.0987\n",
            "     74        \u001b[36m0.2760\u001b[0m  0.0944\n",
            "     75        \u001b[36m0.2676\u001b[0m  0.0941\n",
            "     76        0.2692  0.0933\n",
            "     77        0.2683  0.0986\n",
            "     78        \u001b[36m0.2635\u001b[0m  0.1017\n",
            "     79        \u001b[36m0.2600\u001b[0m  0.1027\n",
            "     80        \u001b[36m0.2569\u001b[0m  0.0938\n",
            "     81        \u001b[36m0.2551\u001b[0m  0.1008\n",
            "     82        \u001b[36m0.2528\u001b[0m  0.0937\n",
            "     83        0.2551  0.1006\n",
            "     84        \u001b[36m0.2515\u001b[0m  0.1062\n",
            "     85        \u001b[36m0.2471\u001b[0m  0.0952\n",
            "     86        \u001b[36m0.2468\u001b[0m  0.0891\n",
            "     87        \u001b[36m0.2451\u001b[0m  0.0910\n",
            "     88        \u001b[36m0.2408\u001b[0m  0.0940\n",
            "     89        0.2412  0.0892\n",
            "     90        0.2431  0.0966\n",
            "     91        \u001b[36m0.2376\u001b[0m  0.1020\n",
            "     92        0.2377  0.0980\n",
            "     93        \u001b[36m0.2329\u001b[0m  0.0968\n",
            "     94        \u001b[36m0.2251\u001b[0m  0.1074\n",
            "     95        \u001b[36m0.2233\u001b[0m  0.0907\n",
            "     96        \u001b[36m0.2187\u001b[0m  0.0921\n",
            "     97        0.2200  0.0916\n",
            "     98        \u001b[36m0.2166\u001b[0m  0.0960\n",
            "     99        \u001b[36m0.2137\u001b[0m  0.0900\n",
            "    100        \u001b[36m0.2130\u001b[0m  0.1005\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0961\n",
            "      2       37.3047  0.1024\n",
            "      3       37.3047  0.1101\n",
            "      4       37.3047  0.0986\n",
            "      5       37.3047  0.0954\n",
            "      6       37.3047  0.1141\n",
            "      7       37.3047  0.1075\n",
            "      8       37.3047  0.1078\n",
            "      9       37.3047  0.1022\n",
            "     10       37.3047  0.1067\n",
            "     11       37.3047  0.1053\n",
            "     12       37.3047  0.1095\n",
            "     13       37.3047  0.0737\n",
            "     14       37.3047  0.0793\n",
            "     15       37.3047  0.0718\n",
            "     16       37.3047  0.0699\n",
            "     17       37.3047  0.0705\n",
            "     18       37.3047  0.0747\n",
            "     19       37.3047  0.0695\n",
            "     20       37.3047  0.0718\n",
            "     21       37.3047  0.0713\n",
            "     22       37.3047  0.0775\n",
            "     23       37.3047  0.0709\n",
            "     24       37.3047  0.0717\n",
            "     25       37.3047  0.0798\n",
            "     26       37.3047  0.0750\n",
            "     27       37.3047  0.0759\n",
            "     28       37.3047  0.0732\n",
            "     29       37.3047  0.0681\n",
            "     30       37.3047  0.0690\n",
            "     31       37.3047  0.0731\n",
            "     32       37.3047  0.0767\n",
            "     33       37.3047  0.0711\n",
            "     34       37.3047  0.0745\n",
            "     35       37.3047  0.0748\n",
            "     36       37.3047  0.0734\n",
            "     37       37.3047  0.0692\n",
            "     38       37.3047  0.0720\n",
            "     39       37.3047  0.0829\n",
            "     40       37.3047  0.0699\n",
            "     41       37.3047  0.0768\n",
            "     42       37.3047  0.0716\n",
            "     43       37.3047  0.0719\n",
            "     44       37.3047  0.0704\n",
            "     45       37.3047  0.0774\n",
            "     46       37.3047  0.0719\n",
            "     47       37.3047  0.0725\n",
            "     48       37.3047  0.0725\n",
            "     49       37.3047  0.0741\n",
            "     50       37.3047  0.0794\n",
            "     51       37.3047  0.0732\n",
            "     52       37.3047  0.0824\n",
            "     53       37.3047  0.0745\n",
            "     54       37.3047  0.0708\n",
            "     55       37.3047  0.0712\n",
            "     56       37.3047  0.0716\n",
            "     57       37.3047  0.0758\n",
            "     58       37.3047  0.0799\n",
            "     59       37.3047  0.0711\n",
            "     60       37.3047  0.0732\n",
            "     61       37.3047  0.0734\n",
            "     62       37.3047  0.0732\n",
            "     63       37.3047  0.0778\n",
            "     64       37.3047  0.0726\n",
            "     65       37.3047  0.0722\n",
            "     66       37.3047  0.0833\n",
            "     67       37.3047  0.0719\n",
            "     68       37.3047  0.0757\n",
            "     69       37.3047  0.0730\n",
            "     70       37.3047  0.0731\n",
            "     71       37.3047  0.0741\n",
            "     72       37.3047  0.0688\n",
            "     73       37.3047  0.0734\n",
            "     74       37.3047  0.0736\n",
            "     75       37.3047  0.0752\n",
            "     76       37.3047  0.0731\n",
            "     77       37.3047  0.0764\n",
            "     78       37.3047  0.0815\n",
            "     79       37.3047  0.0806\n",
            "     80       37.3047  0.0728\n",
            "     81       37.3047  0.0701\n",
            "     82       37.3047  0.0789\n",
            "     83       37.3047  0.0788\n",
            "     84       37.3047  0.3062\n",
            "     85       37.3047  0.1874\n",
            "     86       37.3047  0.2769\n",
            "     87       37.3047  0.3026\n",
            "     88       37.3047  0.1878\n",
            "     89       37.3047  0.1716\n",
            "     90       37.3047  0.0747\n",
            "     91       37.3047  0.0725\n",
            "     92       37.3047  0.0708\n",
            "     93       37.3047  0.0856\n",
            "     94       37.3047  0.0769\n",
            "     95       37.3047  0.0749\n",
            "     96       37.3047  0.0731\n",
            "     97       37.3047  0.0755\n",
            "     98       37.3047  0.0769\n",
            "     99       37.3047  0.0723\n",
            "    100       37.3047  0.0709\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0684\n",
            "      2       37.3047  0.0715\n",
            "      3       37.3047  0.0742\n",
            "      4       37.3047  0.0697\n",
            "      5       37.3047  0.0717\n",
            "      6       37.3047  0.0808\n",
            "      7       37.3047  0.0759\n",
            "      8       37.3047  0.0708\n",
            "      9       37.3047  0.0709\n",
            "     10       37.3047  0.0772\n",
            "     11       37.3047  0.0850\n",
            "     12       37.3047  0.0742\n",
            "     13       37.3047  0.0759\n",
            "     14       37.3047  0.0755\n",
            "     15       37.3047  0.0730\n",
            "     16       37.3047  0.0914\n",
            "     17       37.3047  0.0834\n",
            "     18       37.3047  0.0730\n",
            "     19       37.3047  0.0797\n",
            "     20       37.3047  0.0767\n",
            "     21       37.3047  0.0678\n",
            "     22       37.3047  0.0713\n",
            "     23       37.3047  0.0765\n",
            "     24       37.3047  0.0747\n",
            "     25       \u001b[36m16.7994\u001b[0m  0.0712\n",
            "     26        \u001b[36m0.5559\u001b[0m  0.0719\n",
            "     27        \u001b[36m0.5289\u001b[0m  0.0701\n",
            "     28        \u001b[36m0.4969\u001b[0m  0.0731\n",
            "     29        \u001b[36m0.4797\u001b[0m  0.0777\n",
            "     30        \u001b[36m0.4650\u001b[0m  0.0721\n",
            "     31        \u001b[36m0.4475\u001b[0m  0.1027\n",
            "     32        \u001b[36m0.4371\u001b[0m  0.1169\n",
            "     33        \u001b[36m0.4356\u001b[0m  0.0967\n",
            "     34        \u001b[36m0.4119\u001b[0m  0.0916\n",
            "     35        \u001b[36m0.3729\u001b[0m  0.0960\n",
            "     36        \u001b[36m0.3566\u001b[0m  0.0916\n",
            "     37        0.3636  0.0893\n",
            "     38        \u001b[36m0.3558\u001b[0m  0.0949\n",
            "     39        0.3590  0.0955\n",
            "     40        \u001b[36m0.3356\u001b[0m  0.0935\n",
            "     41        \u001b[36m0.3095\u001b[0m  0.0983\n",
            "     42        0.3327  0.0968\n",
            "     43        \u001b[36m0.3023\u001b[0m  0.1055\n",
            "     44        0.3053  0.0948\n",
            "     45        \u001b[36m0.2976\u001b[0m  0.0962\n",
            "     46        0.3034  0.1057\n",
            "     47        \u001b[36m0.2709\u001b[0m  0.1013\n",
            "     48        0.3026  0.0983\n",
            "     49        0.2727  0.1149\n",
            "     50        0.2803  0.1001\n",
            "     51        \u001b[36m0.2483\u001b[0m  0.0935\n",
            "     52        0.2783  0.0941\n",
            "     53        0.2570  0.1014\n",
            "     54        0.2531  0.0939\n",
            "     55        0.2519  0.1109\n",
            "     56        \u001b[36m0.2357\u001b[0m  0.1022\n",
            "     57        0.2875  0.0958\n",
            "     58        \u001b[36m0.2339\u001b[0m  0.1025\n",
            "     59        \u001b[36m0.2220\u001b[0m  0.0965\n",
            "     60        0.2432  0.1026\n",
            "     61        0.2337  0.0961\n",
            "     62        0.2542  0.0984\n",
            "     63        \u001b[36m0.2157\u001b[0m  0.0963\n",
            "     64        \u001b[36m0.2108\u001b[0m  0.1055\n",
            "     65        0.2182  0.1008\n",
            "     66        \u001b[36m0.2058\u001b[0m  0.1017\n",
            "     67        0.2071  0.0967\n",
            "     68        0.2141  0.0999\n",
            "     69        \u001b[36m0.1983\u001b[0m  0.1103\n",
            "     70        0.2005  0.1053\n",
            "     71        \u001b[36m0.1939\u001b[0m  0.1056\n",
            "     72        \u001b[36m0.1784\u001b[0m  0.1092\n",
            "     73        0.1833  0.1073\n",
            "     74        0.1918  0.1117\n",
            "     75        0.1927  0.1072\n",
            "     76        0.1906  0.0798\n",
            "     77        0.1896  0.0858\n",
            "     78        0.1844  0.0843\n",
            "     79        \u001b[36m0.1758\u001b[0m  0.0765\n",
            "     80        \u001b[36m0.1706\u001b[0m  0.0757\n",
            "     81        0.1764  0.0806\n",
            "     82        \u001b[36m0.1690\u001b[0m  0.0735\n",
            "     83        \u001b[36m0.1671\u001b[0m  0.0721\n",
            "     84        \u001b[36m0.1619\u001b[0m  0.0816\n",
            "     85        0.1669  0.0773\n",
            "     86        0.1676  0.0737\n",
            "     87        0.1621  0.0770\n",
            "     88        0.1635  0.0680\n",
            "     89        \u001b[36m0.1573\u001b[0m  0.0718\n",
            "     90        0.1576  0.0736\n",
            "     91        \u001b[36m0.1521\u001b[0m  0.0728\n",
            "     92        0.1661  0.0720\n",
            "     93        0.1610  0.0710\n",
            "     94        0.1696  0.0786\n",
            "     95        0.1616  0.0728\n",
            "     96        0.1550  0.0723\n",
            "     97        0.1525  0.0790\n",
            "     98        \u001b[36m0.1507\u001b[0m  0.0750\n",
            "     99        \u001b[36m0.1481\u001b[0m  0.0733\n",
            "    100        \u001b[36m0.1473\u001b[0m  0.0722\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0694\n",
            "      2       37.3047  0.0737\n",
            "      3       37.3047  0.0739\n",
            "      4       37.3047  0.0750\n",
            "      5       37.3047  0.0778\n",
            "      6       37.3047  0.0734\n",
            "      7       37.3047  0.0753\n",
            "      8       37.3047  0.0720\n",
            "      9       37.3047  0.0710\n",
            "     10       37.3047  0.0690\n",
            "     11       37.3047  0.0846\n",
            "     12       37.3047  0.0707\n",
            "     13       37.3047  0.0714\n",
            "     14       37.3047  0.0735\n",
            "     15       37.3047  0.0786\n",
            "     16       37.3047  0.0745\n",
            "     17       37.3047  0.0776\n",
            "     18       37.3047  0.0739\n",
            "     19       37.3047  0.0727\n",
            "     20       37.3047  0.0766\n",
            "     21       37.3047  0.0724\n",
            "     22       37.3047  0.0727\n",
            "     23       37.3047  0.0716\n",
            "     24       37.3047  0.0801\n",
            "     25       37.3047  0.0760\n",
            "     26       \u001b[36m16.3706\u001b[0m  0.0702\n",
            "     27        \u001b[36m0.6215\u001b[0m  0.0709\n",
            "     28        \u001b[36m0.5442\u001b[0m  0.0709\n",
            "     29        \u001b[36m0.5279\u001b[0m  0.0769\n",
            "     30        \u001b[36m0.5083\u001b[0m  0.0727\n",
            "     31        \u001b[36m0.4928\u001b[0m  0.0690\n",
            "     32        \u001b[36m0.4679\u001b[0m  0.0703\n",
            "     33        \u001b[36m0.4464\u001b[0m  0.0749\n",
            "     34        \u001b[36m0.4313\u001b[0m  0.0717\n",
            "     35        \u001b[36m0.4165\u001b[0m  0.0736\n",
            "     36        \u001b[36m0.4039\u001b[0m  0.0718\n",
            "     37        \u001b[36m0.3896\u001b[0m  0.0758\n",
            "     38        \u001b[36m0.3795\u001b[0m  0.0849\n",
            "     39        \u001b[36m0.3711\u001b[0m  0.0719\n",
            "     40        \u001b[36m0.3613\u001b[0m  0.0713\n",
            "     41        \u001b[36m0.3509\u001b[0m  0.0767\n",
            "     42        \u001b[36m0.3391\u001b[0m  0.0740\n",
            "     43        \u001b[36m0.3221\u001b[0m  0.0706\n",
            "     44        \u001b[36m0.3166\u001b[0m  0.0803\n",
            "     45        \u001b[36m0.3104\u001b[0m  0.0745\n",
            "     46        \u001b[36m0.3022\u001b[0m  0.0704\n",
            "     47        \u001b[36m0.2930\u001b[0m  0.0703\n",
            "     48        \u001b[36m0.2899\u001b[0m  0.0687\n",
            "     49        \u001b[36m0.2828\u001b[0m  0.0719\n",
            "     50        \u001b[36m0.2799\u001b[0m  0.0714\n",
            "     51        \u001b[36m0.2745\u001b[0m  0.0895\n",
            "     52        \u001b[36m0.2684\u001b[0m  0.0780\n",
            "     53        \u001b[36m0.2638\u001b[0m  0.0753\n",
            "     54        \u001b[36m0.2590\u001b[0m  0.0727\n",
            "     55        \u001b[36m0.2534\u001b[0m  0.0762\n",
            "     56        \u001b[36m0.2510\u001b[0m  0.0720\n",
            "     57        \u001b[36m0.2472\u001b[0m  0.0704\n",
            "     58        \u001b[36m0.2373\u001b[0m  0.0738\n",
            "     59        \u001b[36m0.2359\u001b[0m  0.0728\n",
            "     60        \u001b[36m0.2305\u001b[0m  0.0749\n",
            "     61        \u001b[36m0.2276\u001b[0m  0.0719\n",
            "     62        \u001b[36m0.2195\u001b[0m  0.0715\n",
            "     63        0.2206  0.0744\n",
            "     64        \u001b[36m0.2118\u001b[0m  0.0719\n",
            "     65        \u001b[36m0.2085\u001b[0m  0.0837\n",
            "     66        \u001b[36m0.2077\u001b[0m  0.0751\n",
            "     67        \u001b[36m0.2054\u001b[0m  0.0764\n",
            "     68        \u001b[36m0.1988\u001b[0m  0.0772\n",
            "     69        0.2043  0.0751\n",
            "     70        \u001b[36m0.1962\u001b[0m  0.0713\n",
            "     71        0.2021  0.0751\n",
            "     72        \u001b[36m0.1865\u001b[0m  0.0721\n",
            "     73        0.2058  0.0741\n",
            "     74        0.1961  0.0738\n",
            "     75        \u001b[36m0.1831\u001b[0m  0.0716\n",
            "     76        0.1915  0.0719\n",
            "     77        0.1915  0.0748\n",
            "     78        0.1865  0.0851\n",
            "     79        \u001b[36m0.1808\u001b[0m  0.0770\n",
            "     80        \u001b[36m0.1711\u001b[0m  0.0733\n",
            "     81        0.1853  0.0730\n",
            "     82        0.1868  0.0755\n",
            "     83        0.1756  0.0717\n",
            "     84        0.1775  0.0845\n",
            "     85        0.1731  0.0783\n",
            "     86        \u001b[36m0.1644\u001b[0m  0.0767\n",
            "     87        0.1658  0.0700\n",
            "     88        0.1654  0.0705\n",
            "     89        0.1665  0.0720\n",
            "     90        \u001b[36m0.1616\u001b[0m  0.0743\n",
            "     91        0.1647  0.0837\n",
            "     92        \u001b[36m0.1602\u001b[0m  0.0726\n",
            "     93        0.1682  0.0741\n",
            "     94        0.1944  0.0714\n",
            "     95        0.1915  0.0714\n",
            "     96        0.1847  0.0760\n",
            "     97        0.1977  0.0788\n",
            "     98        0.1769  0.0835\n",
            "     99        0.1806  0.0753\n",
            "    100        0.1769  0.0712\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0995\n",
            "      2       37.3047  0.1116\n",
            "      3       37.3047  0.1126\n",
            "      4       37.3047  0.1415\n",
            "      5       37.3047  0.1637\n",
            "      6       37.3047  0.0954\n",
            "      7       37.3047  0.0920\n",
            "      8       37.3047  0.2248\n",
            "      9       37.3047  0.3351\n",
            "     10       37.3047  0.3935\n",
            "     11       37.3047  0.2762\n",
            "     12       37.3047  0.1413\n",
            "     13       37.3047  0.1933\n",
            "     14       37.3047  0.1076\n",
            "     15       37.3047  0.1218\n",
            "     16       37.3047  0.0975\n",
            "     17       37.3047  0.1102\n",
            "     18       37.3047  0.1068\n",
            "     19       37.3047  0.0921\n",
            "     20       37.3047  0.1633\n",
            "     21       37.3047  0.1262\n",
            "     22       37.3047  0.1704\n",
            "     23       37.3047  0.2318\n",
            "     24       37.3047  0.3060\n",
            "     25       \u001b[36m16.6427\u001b[0m  0.2695\n",
            "     26        \u001b[36m0.5807\u001b[0m  0.2895\n",
            "     27        \u001b[36m0.5379\u001b[0m  0.2965\n",
            "     28        \u001b[36m0.5118\u001b[0m  0.2869\n",
            "     29        \u001b[36m0.4981\u001b[0m  0.2991\n",
            "     30        \u001b[36m0.4801\u001b[0m  0.1990\n",
            "     31        \u001b[36m0.4649\u001b[0m  0.2145\n",
            "     32        \u001b[36m0.4491\u001b[0m  0.1697\n",
            "     33        \u001b[36m0.4314\u001b[0m  0.1466\n",
            "     34        \u001b[36m0.4154\u001b[0m  0.2240\n",
            "     35        \u001b[36m0.4032\u001b[0m  0.1141\n",
            "     36        \u001b[36m0.3930\u001b[0m  0.1479\n",
            "     37        \u001b[36m0.3800\u001b[0m  0.1690\n",
            "     38        \u001b[36m0.3687\u001b[0m  0.1531\n",
            "     39        \u001b[36m0.3643\u001b[0m  0.1684\n",
            "     40        \u001b[36m0.3595\u001b[0m  0.1879\n",
            "     41        \u001b[36m0.3456\u001b[0m  0.1395\n",
            "     42        0.3540  0.2338\n",
            "     43        \u001b[36m0.3225\u001b[0m  0.1400\n",
            "     44        0.3326  0.1607\n",
            "     45        \u001b[36m0.3116\u001b[0m  0.1507\n",
            "     46        0.3235  0.1935\n",
            "     47        \u001b[36m0.3110\u001b[0m  0.0800\n",
            "     48        \u001b[36m0.2934\u001b[0m  0.0741\n",
            "     49        0.3044  0.1034\n",
            "     50        \u001b[36m0.2826\u001b[0m  0.1557\n",
            "     51        0.2979  0.1219\n",
            "     52        \u001b[36m0.2798\u001b[0m  0.1168\n",
            "     53        \u001b[36m0.2737\u001b[0m  0.1145\n",
            "     54        \u001b[36m0.2735\u001b[0m  0.1192\n",
            "     55        \u001b[36m0.2631\u001b[0m  0.1346\n",
            "     56        \u001b[36m0.2613\u001b[0m  0.1216\n",
            "     57        \u001b[36m0.2571\u001b[0m  0.1153\n",
            "     58        \u001b[36m0.2507\u001b[0m  0.1172\n",
            "     59        \u001b[36m0.2485\u001b[0m  0.1258\n",
            "     60        \u001b[36m0.2442\u001b[0m  0.1189\n",
            "     61        \u001b[36m0.2334\u001b[0m  0.1144\n",
            "     62        \u001b[36m0.2286\u001b[0m  0.1189\n",
            "     63        0.2348  0.1273\n",
            "     64        0.2377  0.1141\n",
            "     65        0.2398  0.1333\n",
            "     66        0.2286  0.1118\n",
            "     67        \u001b[36m0.2279\u001b[0m  0.1157\n",
            "     68        \u001b[36m0.2242\u001b[0m  0.1203\n",
            "     69        \u001b[36m0.2223\u001b[0m  0.2304\n",
            "     70        \u001b[36m0.2128\u001b[0m  0.3351\n",
            "     71        0.2175  0.1405\n",
            "     72        0.2162  0.1041\n",
            "     73        0.2216  0.0775\n",
            "     74        \u001b[36m0.1923\u001b[0m  0.0703\n",
            "     75        0.2080  0.0733\n",
            "     76        0.2683  0.0786\n",
            "     77        0.2146  0.0799\n",
            "     78        0.2148  0.0770\n",
            "     79        0.1975  0.0820\n",
            "     80        0.1984  0.0689\n",
            "     81        0.1936  0.0703\n",
            "     82        0.1985  0.0770\n",
            "     83        \u001b[36m0.1893\u001b[0m  0.0731\n",
            "     84        \u001b[36m0.1837\u001b[0m  0.0739\n",
            "     85        \u001b[36m0.1813\u001b[0m  0.0708\n",
            "     86        0.1873  0.0760\n",
            "     87        0.1838  0.0727\n",
            "     88        \u001b[36m0.1813\u001b[0m  0.0783\n",
            "     89        \u001b[36m0.1750\u001b[0m  0.0772\n",
            "     90        \u001b[36m0.1737\u001b[0m  0.0755\n",
            "     91        \u001b[36m0.1703\u001b[0m  0.0729\n",
            "     92        \u001b[36m0.1696\u001b[0m  0.1294\n",
            "     93        \u001b[36m0.1651\u001b[0m  0.0773\n",
            "     94        \u001b[36m0.1643\u001b[0m  0.0737\n",
            "     95        \u001b[36m0.1568\u001b[0m  0.1018\n",
            "     96        \u001b[36m0.1551\u001b[0m  0.1729\n",
            "     97        \u001b[36m0.1547\u001b[0m  0.1539\n",
            "     98        0.1606  0.1494\n",
            "     99        0.1612  0.1698\n",
            "    100        \u001b[36m0.1500\u001b[0m  0.1320\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.1626\n",
            "      2       37.3047  0.1293\n",
            "      3       37.3047  0.0867\n",
            "      4       37.3047  0.0806\n",
            "      5       37.3047  0.0835\n",
            "      6       37.3047  0.1270\n",
            "      7       37.3047  0.2168\n",
            "      8       37.3047  0.1850\n",
            "      9       37.3047  0.0939\n",
            "     10       37.3047  0.0928\n",
            "     11       37.3047  0.2041\n",
            "     12       37.3047  0.3806\n",
            "     13       37.3047  0.2313\n",
            "     14       37.3047  0.1390\n",
            "     15       37.3047  0.0990\n",
            "     16       37.3047  0.1285\n",
            "     17       37.3047  0.1027\n",
            "     18       37.3047  0.1090\n",
            "     19       37.3047  0.1657\n",
            "     20       37.3047  0.1009\n",
            "     21       37.3047  0.1976\n",
            "     22       37.3047  0.1070\n",
            "     23       37.3047  0.0954\n",
            "     24       37.3047  0.1630\n",
            "     25       \u001b[36m16.4857\u001b[0m  0.2170\n",
            "     26        \u001b[36m0.5531\u001b[0m  0.2930\n",
            "     27        \u001b[36m0.5169\u001b[0m  0.2269\n",
            "     28        \u001b[36m0.4972\u001b[0m  0.2299\n",
            "     29        \u001b[36m0.4941\u001b[0m  0.1526\n",
            "     30        \u001b[36m0.4795\u001b[0m  0.2157\n",
            "     31        \u001b[36m0.4665\u001b[0m  0.2873\n",
            "     32        \u001b[36m0.4529\u001b[0m  0.5144\n",
            "     33        \u001b[36m0.4407\u001b[0m  0.4108\n",
            "     34        \u001b[36m0.4235\u001b[0m  0.2128\n",
            "     35        \u001b[36m0.4150\u001b[0m  0.2716\n",
            "     36        \u001b[36m0.3986\u001b[0m  0.2263\n",
            "     37        \u001b[36m0.3863\u001b[0m  0.2552\n",
            "     38        \u001b[36m0.3662\u001b[0m  0.1767\n",
            "     39        \u001b[36m0.3563\u001b[0m  0.2293\n",
            "     40        \u001b[36m0.3528\u001b[0m  0.2534\n",
            "     41        \u001b[36m0.3424\u001b[0m  0.1766\n",
            "     42        \u001b[36m0.3295\u001b[0m  0.1286\n",
            "     43        \u001b[36m0.3227\u001b[0m  0.1329\n",
            "     44        \u001b[36m0.3220\u001b[0m  0.0859\n",
            "     45        \u001b[36m0.3196\u001b[0m  0.0699\n",
            "     46        0.3320  0.0731\n",
            "     47        \u001b[36m0.3167\u001b[0m  0.0751\n",
            "     48        \u001b[36m0.2986\u001b[0m  0.0730\n",
            "     49        0.3025  0.0721\n",
            "     50        0.3066  0.0739\n",
            "     51        0.3142  0.0728\n",
            "     52        \u001b[36m0.2919\u001b[0m  0.0746\n",
            "     53        0.2971  0.0721\n",
            "     54        \u001b[36m0.2864\u001b[0m  0.0796\n",
            "     55        \u001b[36m0.2793\u001b[0m  0.0740\n",
            "     56        \u001b[36m0.2739\u001b[0m  0.0757\n",
            "     57        \u001b[36m0.2650\u001b[0m  0.0751\n",
            "     58        \u001b[36m0.2544\u001b[0m  0.0732\n",
            "     59        \u001b[36m0.2448\u001b[0m  0.0733\n",
            "     60        0.2449  0.0727\n",
            "     61        \u001b[36m0.2345\u001b[0m  0.0738\n",
            "     62        0.2497  0.0777\n",
            "     63        0.2364  0.0752\n",
            "     64        \u001b[36m0.2333\u001b[0m  0.0734\n",
            "     65        \u001b[36m0.2269\u001b[0m  0.0755\n",
            "     66        \u001b[36m0.2243\u001b[0m  0.0740\n",
            "     67        \u001b[36m0.2179\u001b[0m  0.0784\n",
            "     68        \u001b[36m0.2142\u001b[0m  0.0724\n",
            "     69        \u001b[36m0.2109\u001b[0m  0.0734\n",
            "     70        \u001b[36m0.2103\u001b[0m  0.0823\n",
            "     71        \u001b[36m0.2085\u001b[0m  0.0697\n",
            "     72        \u001b[36m0.1935\u001b[0m  0.0758\n",
            "     73        \u001b[36m0.1920\u001b[0m  0.0715\n",
            "     74        0.1991  0.0803\n",
            "     75        0.2012  0.0736\n",
            "     76        0.1991  0.0711\n",
            "     77        0.1997  0.0707\n",
            "     78        0.1947  0.0712\n",
            "     79        \u001b[36m0.1902\u001b[0m  0.0753\n",
            "     80        0.1997  0.0738\n",
            "     81        0.1998  0.0806\n",
            "     82        0.1969  0.0728\n",
            "     83        \u001b[36m0.1821\u001b[0m  0.0742\n",
            "     84        0.1998  0.0844\n",
            "     85        0.1994  0.0731\n",
            "     86        0.2003  0.0723\n",
            "     87        0.2095  0.0694\n",
            "     88        0.2127  0.0697\n",
            "     89        0.2018  0.0744\n",
            "     90        0.1972  0.0770\n",
            "     91        \u001b[36m0.1800\u001b[0m  0.0732\n",
            "     92        0.1919  0.0744\n",
            "     93        0.1831  0.0710\n",
            "     94        \u001b[36m0.1753\u001b[0m  0.0811\n",
            "     95        0.1834  0.0717\n",
            "     96        \u001b[36m0.1733\u001b[0m  0.0734\n",
            "     97        0.1777  0.0746\n",
            "     98        \u001b[36m0.1701\u001b[0m  0.0755\n",
            "     99        \u001b[36m0.1693\u001b[0m  0.0760\n",
            "    100        \u001b[36m0.1647\u001b[0m  0.0714\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0673\n",
            "      2       37.3047  0.0743\n",
            "      3       37.3047  0.0728\n",
            "      4       37.3047  0.0699\n",
            "      5       37.3047  0.0702\n",
            "      6       37.3047  0.0729\n",
            "      7       37.3047  0.0707\n",
            "      8       37.3047  0.0841\n",
            "      9       37.3047  0.0703\n",
            "     10       37.3047  0.0766\n",
            "     11       37.3047  0.0709\n",
            "     12       37.3047  0.0731\n",
            "     13       37.3047  0.0700\n",
            "     14       37.3047  0.0737\n",
            "     15       37.3047  0.0691\n",
            "     16       37.3047  0.0673\n",
            "     17       37.3047  0.0696\n",
            "     18       37.3047  0.0731\n",
            "     19       37.3047  0.0769\n",
            "     20       37.3047  0.0727\n",
            "     21       37.3047  0.0770\n",
            "     22       37.3047  0.0808\n",
            "     23       37.3047  0.0783\n",
            "     24       \u001b[36m16.2269\u001b[0m  0.0739\n",
            "     25        \u001b[36m0.5315\u001b[0m  0.0715\n",
            "     26        \u001b[36m0.5171\u001b[0m  0.0763\n",
            "     27        \u001b[36m0.4967\u001b[0m  0.0706\n",
            "     28        \u001b[36m0.4802\u001b[0m  0.0746\n",
            "     29        \u001b[36m0.4682\u001b[0m  0.0716\n",
            "     30        \u001b[36m0.4530\u001b[0m  0.0728\n",
            "     31        \u001b[36m0.4421\u001b[0m  0.0715\n",
            "     32        \u001b[36m0.4341\u001b[0m  0.0732\n",
            "     33        \u001b[36m0.4260\u001b[0m  0.0773\n",
            "     34        \u001b[36m0.4151\u001b[0m  0.0736\n",
            "     35        \u001b[36m0.4010\u001b[0m  0.0818\n",
            "     36        \u001b[36m0.3913\u001b[0m  0.0755\n",
            "     37        \u001b[36m0.3779\u001b[0m  0.0788\n",
            "     38        \u001b[36m0.3676\u001b[0m  0.0762\n",
            "     39        \u001b[36m0.3550\u001b[0m  0.0744\n",
            "     40        \u001b[36m0.3446\u001b[0m  0.0746\n",
            "     41        \u001b[36m0.3391\u001b[0m  0.0735\n",
            "     42        \u001b[36m0.3311\u001b[0m  0.0767\n",
            "     43        \u001b[36m0.3230\u001b[0m  0.0726\n",
            "     44        \u001b[36m0.3165\u001b[0m  0.0737\n",
            "     45        \u001b[36m0.3113\u001b[0m  0.0754\n",
            "     46        \u001b[36m0.3074\u001b[0m  0.0735\n",
            "     47        \u001b[36m0.3021\u001b[0m  0.1047\n",
            "     48        \u001b[36m0.2990\u001b[0m  0.1094\n",
            "     49        \u001b[36m0.2851\u001b[0m  0.1014\n",
            "     50        0.2877  0.1044\n",
            "     51        \u001b[36m0.2751\u001b[0m  0.0995\n",
            "     52        0.2778  0.0984\n",
            "     53        \u001b[36m0.2644\u001b[0m  0.1005\n",
            "     54        0.2693  0.0938\n",
            "     55        \u001b[36m0.2552\u001b[0m  0.1000\n",
            "     56        0.2619  0.0995\n",
            "     57        \u001b[36m0.2445\u001b[0m  0.0908\n",
            "     58        0.2496  0.1004\n",
            "     59        \u001b[36m0.2388\u001b[0m  0.0954\n",
            "     60        \u001b[36m0.2340\u001b[0m  0.0976\n",
            "     61        0.2421  0.0998\n",
            "     62        \u001b[36m0.2294\u001b[0m  0.1023\n",
            "     63        \u001b[36m0.2262\u001b[0m  0.1032\n",
            "     64        0.2344  0.1026\n",
            "     65        \u001b[36m0.2196\u001b[0m  0.0974\n",
            "     66        0.2314  0.0918\n",
            "     67        \u001b[36m0.2155\u001b[0m  0.0967\n",
            "     68        0.2218  0.1129\n",
            "     69        \u001b[36m0.2120\u001b[0m  0.0937\n",
            "     70        0.2168  0.0970\n",
            "     71        \u001b[36m0.2071\u001b[0m  0.0991\n",
            "     72        0.2088  0.0912\n",
            "     73        \u001b[36m0.2064\u001b[0m  0.0957\n",
            "     74        \u001b[36m0.2022\u001b[0m  0.0921\n",
            "     75        \u001b[36m0.2011\u001b[0m  0.0966\n",
            "     76        \u001b[36m0.1979\u001b[0m  0.0898\n",
            "     77        \u001b[36m0.1965\u001b[0m  0.0905\n",
            "     78        \u001b[36m0.1902\u001b[0m  0.0973\n",
            "     79        0.1920  0.1088\n",
            "     80        \u001b[36m0.1850\u001b[0m  0.0911\n",
            "     81        0.1945  0.0994\n",
            "     82        \u001b[36m0.1790\u001b[0m  0.0947\n",
            "     83        0.1812  0.1031\n",
            "     84        0.1819  0.0997\n",
            "     85        \u001b[36m0.1747\u001b[0m  0.1034\n",
            "     86        \u001b[36m0.1733\u001b[0m  0.1067\n",
            "     87        \u001b[36m0.1693\u001b[0m  0.1046\n",
            "     88        0.1767  0.1067\n",
            "     89        0.1696  0.1109\n",
            "     90        \u001b[36m0.1643\u001b[0m  0.0982\n",
            "     91        \u001b[36m0.1631\u001b[0m  0.1052\n",
            "     92        \u001b[36m0.1606\u001b[0m  0.0936\n",
            "     93        0.1611  0.0735\n",
            "     94        \u001b[36m0.1588\u001b[0m  0.0712\n",
            "     95        0.1620  0.0700\n",
            "     96        \u001b[36m0.1572\u001b[0m  0.0712\n",
            "     97        \u001b[36m0.1564\u001b[0m  0.0682\n",
            "     98        \u001b[36m0.1528\u001b[0m  0.0771\n",
            "     99        \u001b[36m0.1499\u001b[0m  0.0730\n",
            "    100        \u001b[36m0.1489\u001b[0m  0.0726\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.2320\u001b[0m  0.0757\n",
            "      2       37.2320  0.0701\n",
            "      3       37.2320  0.0682\n",
            "      4       37.2320  0.0745\n",
            "      5       37.2320  0.0694\n",
            "      6       37.2320  0.0697\n",
            "      7       37.2320  0.0707\n",
            "      8       37.2320  0.0726\n",
            "      9       37.2320  0.0705\n",
            "     10       37.2320  0.0754\n",
            "     11       37.2320  0.0706\n",
            "     12       37.2320  0.0702\n",
            "     13       37.2320  0.0726\n",
            "     14       37.2320  0.0720\n",
            "     15       37.2320  0.0865\n",
            "     16       37.2320  0.0754\n",
            "     17       37.2320  0.0721\n",
            "     18       37.2320  0.0706\n",
            "     19       37.2320  0.0691\n",
            "     20       37.2320  0.0719\n",
            "     21       37.2320  0.0726\n",
            "     22       37.2320  0.0726\n",
            "     23       37.2320  0.0727\n",
            "     24       \u001b[36m18.9361\u001b[0m  0.0708\n",
            "     25        \u001b[36m0.5358\u001b[0m  0.0791\n",
            "     26        \u001b[36m0.5184\u001b[0m  0.0754\n",
            "     27        0.5234  0.0753\n",
            "     28        \u001b[36m0.4847\u001b[0m  0.0890\n",
            "     29        \u001b[36m0.4690\u001b[0m  0.0799\n",
            "     30        0.4755  0.0756\n",
            "     31        \u001b[36m0.4412\u001b[0m  0.0716\n",
            "     32        \u001b[36m0.4337\u001b[0m  0.0705\n",
            "     33        \u001b[36m0.4147\u001b[0m  0.0678\n",
            "     34        \u001b[36m0.3823\u001b[0m  0.0734\n",
            "     35        0.3993  0.0764\n",
            "     36        \u001b[36m0.3583\u001b[0m  0.0721\n",
            "     37        0.3590  0.0774\n",
            "     38        \u001b[36m0.3378\u001b[0m  0.0697\n",
            "     39        \u001b[36m0.3242\u001b[0m  0.0728\n",
            "     40        \u001b[36m0.3161\u001b[0m  0.0754\n",
            "     41        \u001b[36m0.3094\u001b[0m  0.0808\n",
            "     42        \u001b[36m0.3005\u001b[0m  0.0813\n",
            "     43        \u001b[36m0.2913\u001b[0m  0.0706\n",
            "     44        \u001b[36m0.2811\u001b[0m  0.0787\n",
            "     45        \u001b[36m0.2775\u001b[0m  0.0758\n",
            "     46        \u001b[36m0.2688\u001b[0m  0.0715\n",
            "     47        \u001b[36m0.2660\u001b[0m  0.0730\n",
            "     48        \u001b[36m0.2593\u001b[0m  0.0763\n",
            "     49        \u001b[36m0.2569\u001b[0m  0.0831\n",
            "     50        \u001b[36m0.2466\u001b[0m  0.0793\n",
            "     51        \u001b[36m0.2438\u001b[0m  0.0718\n",
            "     52        \u001b[36m0.2408\u001b[0m  0.0750\n",
            "     53        \u001b[36m0.2316\u001b[0m  0.0753\n",
            "     54        0.2440  0.0737\n",
            "     55        0.2336  0.0791\n",
            "     56        \u001b[36m0.2293\u001b[0m  0.0787\n",
            "     57        0.2370  0.0748\n",
            "     58        0.2293  0.0727\n",
            "     59        \u001b[36m0.2153\u001b[0m  0.0706\n",
            "     60        \u001b[36m0.2045\u001b[0m  0.0733\n",
            "     61        0.2532  0.1365\n",
            "     62        0.2163  0.0916\n",
            "     63        \u001b[36m0.1953\u001b[0m  0.0760\n",
            "     64        0.2086  0.0732\n",
            "     65        \u001b[36m0.1848\u001b[0m  0.0965\n",
            "     66        0.1852  0.1744\n",
            "     67        \u001b[36m0.1770\u001b[0m  0.2209\n",
            "     68        0.1854  0.1042\n",
            "     69        \u001b[36m0.1639\u001b[0m  0.0937\n",
            "     70        0.3074  0.0798\n",
            "     71        0.1688  0.1078\n",
            "     72        0.1640  0.1288\n",
            "     73        0.1646  0.1270\n",
            "     74        \u001b[36m0.1637\u001b[0m  0.1470\n",
            "     75        \u001b[36m0.1597\u001b[0m  0.1565\n",
            "     76        \u001b[36m0.1528\u001b[0m  0.1183\n",
            "     77        \u001b[36m0.1521\u001b[0m  0.1000\n",
            "     78        0.1525  0.0727\n",
            "     79        \u001b[36m0.1471\u001b[0m  0.0748\n",
            "     80        0.1474  0.0949\n",
            "     81        0.1475  0.0965\n",
            "     82        \u001b[36m0.1422\u001b[0m  0.0739\n",
            "     83        \u001b[36m0.1403\u001b[0m  0.0746\n",
            "     84        \u001b[36m0.1401\u001b[0m  0.1163\n",
            "     85        \u001b[36m0.1388\u001b[0m  0.1206\n",
            "     86        \u001b[36m0.1359\u001b[0m  0.1215\n",
            "     87        \u001b[36m0.1299\u001b[0m  0.1422\n",
            "     88        0.1377  0.1162\n",
            "     89        \u001b[36m0.1278\u001b[0m  0.1236\n",
            "     90        0.1313  0.0936\n",
            "     91        0.1296  0.0762\n",
            "     92        0.1310  0.0802\n",
            "     93        \u001b[36m0.1211\u001b[0m  0.2089\n",
            "     94        0.1273  0.1211\n",
            "     95        0.1255  0.0974\n",
            "     96        0.1316  0.0806\n",
            "     97        0.1236  0.0720\n",
            "     98        0.1422  0.0711\n",
            "     99        0.1410  0.0948\n",
            "    100        0.1314  0.1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VCyq8LtFllV",
        "outputId": "0ce98a0d-7f66-4c49-9756-c7f3c3c77a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9YEx7HgFpZm",
        "outputId": "29678f02-8d96-45f2-9a16-9fbe3a986eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61403509, 0.61403509, 0.85964912, 0.63157895, 0.85964912,\n",
              "       0.89473684, 0.84210526, 0.89473684, 0.89473684, 0.875     ])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média\n",
        "media = resultados.mean()\n",
        "media"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3a9PrKXFvqN",
        "outputId": "ac51a4f5-87ef-494d-c56a-f6e24937e36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7980263157894737"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desvio Padrão\n",
        "desvio = resultados.std()\n",
        "desvio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUh14BLbGAnV",
        "outputId": "4c7c0653-df89-42fd-d8a9-cf1e9d1272e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11786928799316641"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 6 - Droupout"
      ],
      "metadata": {
        "id": "ErI-RCxNPoOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O Skorch exige que a RN seja passada por uma classe que herda da classe module do pytorch\n",
        "class classificador_torch(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # 30 -> 16 -> 16 -> 1\n",
        "    # Primeira camada densa\n",
        "    self.dense0 = nn.Linear(30, 16)\n",
        "    # Definindo a inicialização dos pesos\n",
        "    torch.nn.init.uniform_(self.dense0.weight)\n",
        "    # Definindo a função de ativação\n",
        "    self.activation0 = nn.ReLU()\n",
        "    # Definindo o dropout\n",
        "    self.dropout0 = nn.Dropout(0.2)\n",
        "    # Com isso, já criamos a ligação entrada com a primeira hidden layer\n",
        "\n",
        "    # Segunda camada densa\n",
        "    self.dense1 = nn.Linear(16, 16)\n",
        "    torch.nn.init.uniform_(self.dense1.weight)\n",
        "    self.activation1 = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "    # Terceira camada densa\n",
        "    self.dense2 = nn.Linear(16, 1)\n",
        "    torch.nn.init.uniform_(self.dense2.weight)\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X = Inputs que vamos receber\n",
        "\n",
        "    #Realizando a ligação entre as camadas\n",
        "    X = self.dense0(X)\n",
        "    X = self.activation0(X)\n",
        "    X = self.dropout0(X)\n",
        "\n",
        "    X = self.dense1(X)\n",
        "    X = self.activation1(X)\n",
        "    X = self.dropout1(X)\n",
        "\n",
        "    X = self.dense2(X)\n",
        "    X = self.output(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "vGsFOsKbGikX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
        "                                                  criterion=torch.nn.BCELoss,\n",
        "                                                  optimizer=torch.optim.Adam,\n",
        "                                                  lr=0.001,\n",
        "                                                  optimizer__weight_decay=0.0001,\n",
        "                                                  max_epochs=100,\n",
        "                                                  batch_size=10,\n",
        "                                                  train_split=False)\n",
        "# Neste caso, usamos o False no split pois faremos a separação na validação cruzada"
      ],
      "metadata": {
        "id": "rKrVDEIMIHHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = cross_val_score(classificador_sklearn, previsores, classes, cv = 10, scoring = 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlFUcV0tIK6m",
        "outputId": "1e556482-cd14-455b-f5f4-20236c1f8bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.1070\n",
            "      2       37.1094  0.0969\n",
            "      3       37.1094  0.0985\n",
            "      4       37.1094  0.1020\n",
            "      5       37.1094  0.0984\n",
            "      6       37.1094  0.1190\n",
            "      7       37.1094  0.1205\n",
            "      8       37.1094  0.1155\n",
            "      9       37.1094  0.1222\n",
            "     10       37.1094  0.1156\n",
            "     11       37.1094  0.1021\n",
            "     12       37.1094  0.0980\n",
            "     13       37.1094  0.1006\n",
            "     14       37.1094  0.1046\n",
            "     15       37.1094  0.1064\n",
            "     16       37.1094  0.1135\n",
            "     17       37.1094  0.1096\n",
            "     18       37.1094  0.1075\n",
            "     19       37.1094  0.1169\n",
            "     20       37.1094  0.1082\n",
            "     21       37.1094  0.1122\n",
            "     22       37.1094  0.1208\n",
            "     23       37.1094  0.1182\n",
            "     24       37.1094  0.1132\n",
            "     25       37.1094  0.1238\n",
            "     26       37.1094  0.1109\n",
            "     27       37.1094  0.1140\n",
            "     28       37.1094  0.0896\n",
            "     29       \u001b[36m15.9111\u001b[0m  0.0839\n",
            "     30        \u001b[36m0.6192\u001b[0m  0.0801\n",
            "     31        \u001b[36m0.5902\u001b[0m  0.0802\n",
            "     32        \u001b[36m0.5747\u001b[0m  0.0795\n",
            "     33        \u001b[36m0.5558\u001b[0m  0.0869\n",
            "     34        \u001b[36m0.5526\u001b[0m  0.0762\n",
            "     35        \u001b[36m0.5373\u001b[0m  0.0743\n",
            "     36        \u001b[36m0.5196\u001b[0m  0.0839\n",
            "     37        0.5310  0.0785\n",
            "     38        \u001b[36m0.5101\u001b[0m  0.0790\n",
            "     39        \u001b[36m0.5074\u001b[0m  0.0738\n",
            "     40        0.5175  0.0750\n",
            "     41        0.5120  0.0773\n",
            "     42        0.5096  0.0741\n",
            "     43        \u001b[36m0.4954\u001b[0m  0.0733\n",
            "     44        \u001b[36m0.4777\u001b[0m  0.0750\n",
            "     45        0.4794  0.0868\n",
            "     46        \u001b[36m0.4749\u001b[0m  0.0788\n",
            "     47        0.4756  0.0756\n",
            "     48        \u001b[36m0.4525\u001b[0m  0.0732\n",
            "     49        0.4598  0.0863\n",
            "     50        \u001b[36m0.4438\u001b[0m  0.0729\n",
            "     51        0.4537  0.0758\n",
            "     52        \u001b[36m0.4362\u001b[0m  0.0754\n",
            "     53        \u001b[36m0.4153\u001b[0m  0.0767\n",
            "     54        0.4196  0.0776\n",
            "     55        \u001b[36m0.4034\u001b[0m  0.0761\n",
            "     56        0.4361  0.0750\n",
            "     57        0.4191  0.0744\n",
            "     58        0.4050  0.0771\n",
            "     59        \u001b[36m0.4000\u001b[0m  0.0803\n",
            "     60        \u001b[36m0.3923\u001b[0m  0.0785\n",
            "     61        0.4043  0.0806\n",
            "     62        \u001b[36m0.3765\u001b[0m  0.0855\n",
            "     63        \u001b[36m0.3755\u001b[0m  0.0783\n",
            "     64        \u001b[36m0.3664\u001b[0m  0.0787\n",
            "     65        \u001b[36m0.3639\u001b[0m  0.0752\n",
            "     66        0.3686  0.0758\n",
            "     67        0.3799  0.0822\n",
            "     68        0.3770  0.0791\n",
            "     69        0.3654  0.0766\n",
            "     70        \u001b[36m0.3462\u001b[0m  0.0784\n",
            "     71        0.3472  0.0782\n",
            "     72        0.3603  0.0840\n",
            "     73        \u001b[36m0.3352\u001b[0m  0.0807\n",
            "     74        0.3592  0.0865\n",
            "     75        \u001b[36m0.3342\u001b[0m  0.0808\n",
            "     76        \u001b[36m0.3111\u001b[0m  0.0810\n",
            "     77        0.3522  0.0760\n",
            "     78        0.3449  0.0807\n",
            "     79        0.3395  0.0844\n",
            "     80        \u001b[36m0.3087\u001b[0m  0.0783\n",
            "     81        0.3100  0.0751\n",
            "     82        0.3330  0.0776\n",
            "     83        0.3261  0.0824\n",
            "     84        0.3355  0.0845\n",
            "     85        \u001b[36m0.2904\u001b[0m  0.0791\n",
            "     86        0.3198  0.0784\n",
            "     87        0.2969  0.0858\n",
            "     88        0.3069  0.0842\n",
            "     89        0.3306  0.0772\n",
            "     90        \u001b[36m0.2881\u001b[0m  0.0783\n",
            "     91        0.3298  0.0811\n",
            "     92        0.2986  0.0820\n",
            "     93        0.3112  0.0796\n",
            "     94        0.3329  0.0765\n",
            "     95        0.3079  0.0769\n",
            "     96        0.3032  0.0824\n",
            "     97        0.3197  0.0750\n",
            "     98        0.3024  0.0802\n",
            "     99        0.2953  0.0891\n",
            "    100        \u001b[36m0.2828\u001b[0m  0.0800\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.0765\n",
            "      2       37.1094  0.0783\n",
            "      3       37.1094  0.0756\n",
            "      4       37.1094  0.0788\n",
            "      5       37.1094  0.0738\n",
            "      6       37.1094  0.0757\n",
            "      7       37.1094  0.0722\n",
            "      8       37.1094  0.0736\n",
            "      9       37.1094  0.0814\n",
            "     10       37.1094  0.0767\n",
            "     11       37.1094  0.0741\n",
            "     12       37.1094  0.0807\n",
            "     13       37.1094  0.0798\n",
            "     14       37.1094  0.0768\n",
            "     15       37.1094  0.0786\n",
            "     16       37.1094  0.0778\n",
            "     17       37.1094  0.0779\n",
            "     18       \u001b[36m36.7484\u001b[0m  0.0820\n",
            "     19       37.1094  0.0768\n",
            "     20       37.1094  0.0809\n",
            "     21       37.1094  0.0786\n",
            "     22       37.1094  0.0866\n",
            "     23       37.1094  0.0803\n",
            "     24       37.1094  0.0803\n",
            "     25       37.1094  0.0895\n",
            "     26       37.1094  0.0812\n",
            "     27       36.9405  0.0796\n",
            "     28       37.1094  0.0784\n",
            "     29       37.1094  0.0803\n",
            "     30       37.1094  0.0800\n",
            "     31       37.1094  0.0764\n",
            "     32       37.1094  0.0768\n",
            "     33       37.1094  0.0807\n",
            "     34       37.1094  0.0834\n",
            "     35       37.1094  0.0926\n",
            "     36       37.1094  0.0793\n",
            "     37       37.1094  0.0871\n",
            "     38       37.1094  0.0769\n",
            "     39       37.1094  0.0814\n",
            "     40       37.1094  0.0798\n",
            "     41       37.1102  0.0771\n",
            "     42       37.1094  0.0760\n",
            "     43       37.1094  0.0827\n",
            "     44       37.1094  0.0801\n",
            "     45       37.1094  0.0790\n",
            "     46       37.1094  0.0829\n",
            "     47       37.1094  0.0831\n",
            "     48       37.1094  0.0802\n",
            "     49       37.1094  0.0773\n",
            "     50       37.1094  0.0927\n",
            "     51       37.1094  0.1016\n",
            "     52       37.1094  0.1061\n",
            "     53       37.1094  0.1068\n",
            "     54       37.1094  0.1149\n",
            "     55       37.1094  0.1056\n",
            "     56       37.1094  0.1040\n",
            "     57       37.1094  0.1069\n",
            "     58       37.1094  0.1059\n",
            "     59       37.1094  0.1176\n",
            "     60       37.1094  0.1060\n",
            "     61       37.1094  0.1033\n",
            "     62       37.1094  0.1036\n",
            "     63       37.1094  0.1045\n",
            "     64       37.1094  0.1046\n",
            "     65       37.1094  0.1001\n",
            "     66       37.1094  0.1114\n",
            "     67       37.1094  0.1065\n",
            "     68       37.1094  0.1091\n",
            "     69       37.1094  0.1134\n",
            "     70       37.1094  0.1021\n",
            "     71       37.1094  0.0994\n",
            "     72       37.1094  0.1013\n",
            "     73       37.1094  0.1013\n",
            "     74       37.1094  0.1063\n",
            "     75       37.1094  0.1086\n",
            "     76       37.1094  0.1041\n",
            "     77       37.1094  0.1064\n",
            "     78       37.1094  0.1071\n",
            "     79       37.1094  0.1012\n",
            "     80       37.1094  0.1122\n",
            "     81       37.1094  0.1045\n",
            "     82       37.1094  0.1232\n",
            "     83       37.1094  0.1201\n",
            "     84       37.1094  0.1057\n",
            "     85       \u001b[36m36.7374\u001b[0m  0.1078\n",
            "     86       36.8066  0.1057\n",
            "     87       37.1278  0.1187\n",
            "     88       37.1094  0.1211\n",
            "     89       37.1094  0.1135\n",
            "     90       37.1094  0.1149\n",
            "     91       36.7724  0.1122\n",
            "     92       \u001b[36m34.5763\u001b[0m  0.1148\n",
            "     93       36.3966  0.0905\n",
            "     94       35.4547  0.0826\n",
            "     95       35.1534  0.0814\n",
            "     96       \u001b[36m34.5468\u001b[0m  0.0775\n",
            "     97       \u001b[36m33.1987\u001b[0m  0.0847\n",
            "     98       \u001b[36m24.6693\u001b[0m  0.0805\n",
            "     99        \u001b[36m1.2219\u001b[0m  0.0754\n",
            "    100        \u001b[36m0.5908\u001b[0m  0.0749\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0752\n",
            "      2       37.3047  0.0810\n",
            "      3       37.3047  0.0769\n",
            "      4       37.3047  0.0807\n",
            "      5       37.3047  0.0796\n",
            "      6       37.3047  0.0824\n",
            "      7       37.3047  0.0792\n",
            "      8       37.3047  0.0762\n",
            "      9       37.3047  0.0752\n",
            "     10       37.3047  0.0853\n",
            "     11       37.3047  0.0749\n",
            "     12       37.3047  0.0763\n",
            "     13       37.3047  0.0754\n",
            "     14       37.3047  0.0804\n",
            "     15       37.3047  0.0811\n",
            "     16       37.3047  0.0782\n",
            "     17       37.3047  0.0801\n",
            "     18       37.3047  0.0886\n",
            "     19       37.3047  0.0772\n",
            "     20       37.3047  0.0832\n",
            "     21       37.3047  0.0986\n",
            "     22       37.3047  0.0978\n",
            "     23       37.3047  0.0764\n",
            "     24       37.3047  0.0759\n",
            "     25       37.3047  0.0842\n",
            "     26       37.3047  0.0813\n",
            "     27       37.3047  0.0771\n",
            "     28       \u001b[36m14.7183\u001b[0m  0.0799\n",
            "     29        \u001b[36m0.6639\u001b[0m  0.0779\n",
            "     30        \u001b[36m0.6409\u001b[0m  0.0822\n",
            "     31        \u001b[36m0.5797\u001b[0m  0.0795\n",
            "     32        \u001b[36m0.5750\u001b[0m  0.0781\n",
            "     33        \u001b[36m0.5574\u001b[0m  0.0784\n",
            "     34        \u001b[36m0.5228\u001b[0m  0.0931\n",
            "     35        0.5643  0.0836\n",
            "     36        0.5442  0.0786\n",
            "     37        \u001b[36m0.5116\u001b[0m  0.0779\n",
            "     38        \u001b[36m0.5030\u001b[0m  0.0796\n",
            "     39        0.5214  0.0842\n",
            "     40        0.5190  0.0790\n",
            "     41        0.5031  0.0765\n",
            "     42        \u001b[36m0.4764\u001b[0m  0.0773\n",
            "     43        0.5046  0.0809\n",
            "     44        0.4907  0.0762\n",
            "     45        0.4935  0.0770\n",
            "     46        \u001b[36m0.4491\u001b[0m  0.0775\n",
            "     47        0.4653  0.0850\n",
            "     48        0.4553  0.0748\n",
            "     49        \u001b[36m0.4378\u001b[0m  0.0768\n",
            "     50        0.4475  0.0751\n",
            "     51        \u001b[36m0.4291\u001b[0m  0.0799\n",
            "     52        \u001b[36m0.4189\u001b[0m  0.0825\n",
            "     53        0.4392  0.0754\n",
            "     54        \u001b[36m0.4107\u001b[0m  0.0756\n",
            "     55        0.4322  0.0795\n",
            "     56        0.4169  0.0824\n",
            "     57        0.4343  0.0763\n",
            "     58        \u001b[36m0.3769\u001b[0m  0.0940\n",
            "     59        0.4122  0.0773\n",
            "     60        0.3834  0.0853\n",
            "     61        \u001b[36m0.3738\u001b[0m  0.0894\n",
            "     62        \u001b[36m0.3695\u001b[0m  0.0799\n",
            "     63        \u001b[36m0.3662\u001b[0m  0.0775\n",
            "     64        \u001b[36m0.3651\u001b[0m  0.0768\n",
            "     65        0.3658  0.0796\n",
            "     66        \u001b[36m0.3513\u001b[0m  0.0829\n",
            "     67        \u001b[36m0.3147\u001b[0m  0.0862\n",
            "     68        0.3208  0.0809\n",
            "     69        0.3432  0.0829\n",
            "     70        0.3279  0.0780\n",
            "     71        0.3323  0.0755\n",
            "     72        0.3189  0.0945\n",
            "     73        0.3297  0.0843\n",
            "     74        \u001b[36m0.2890\u001b[0m  0.0814\n",
            "     75        0.3187  0.0804\n",
            "     76        0.3035  0.0879\n",
            "     77        0.3220  0.0815\n",
            "     78        0.2931  0.0891\n",
            "     79        0.2925  0.0832\n",
            "     80        0.3103  0.0839\n",
            "     81        0.3004  0.0815\n",
            "     82        \u001b[36m0.2770\u001b[0m  0.0823\n",
            "     83        0.2818  0.0781\n",
            "     84        0.2941  0.0871\n",
            "     85        0.2936  0.0801\n",
            "     86        0.2828  0.0822\n",
            "     87        0.2802  0.0785\n",
            "     88        0.2839  0.0805\n",
            "     89        \u001b[36m0.2726\u001b[0m  0.0770\n",
            "     90        0.2737  0.0844\n",
            "     91        0.2923  0.0802\n",
            "     92        \u001b[36m0.2522\u001b[0m  0.0795\n",
            "     93        0.2595  0.0806\n",
            "     94        0.2775  0.0811\n",
            "     95        0.2576  0.0838\n",
            "     96        0.2797  0.0857\n",
            "     97        0.2754  0.0871\n",
            "     98        0.2723  0.0843\n",
            "     99        0.2638  0.0815\n",
            "    100        0.2602  0.0766\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0753\n",
            "      2       37.3047  0.0740\n",
            "      3       37.3047  0.0763\n",
            "      4       37.3047  0.0747\n",
            "      5       37.3047  0.0781\n",
            "      6       37.3047  0.0769\n",
            "      7       37.3047  0.0757\n",
            "      8       37.3047  0.0757\n",
            "      9       37.3047  0.0919\n",
            "     10       37.3047  0.0794\n",
            "     11       37.3047  0.0784\n",
            "     12       37.3047  0.0795\n",
            "     13       37.3047  0.0814\n",
            "     14       37.3047  0.0945\n",
            "     15       37.3047  0.1086\n",
            "     16       37.3047  0.1069\n",
            "     17       37.3047  0.1069\n",
            "     18       37.3047  0.1043\n",
            "     19       37.3047  0.1034\n",
            "     20       37.3047  0.1254\n",
            "     21       37.3047  0.1031\n",
            "     22       37.3047  0.1085\n",
            "     23       37.3047  0.1030\n",
            "     24       37.3047  0.1064\n",
            "     25       37.3047  0.0966\n",
            "     26       37.3047  0.1001\n",
            "     27       \u001b[36m37.1358\u001b[0m  0.0982\n",
            "     28        \u001b[36m6.2769\u001b[0m  0.1081\n",
            "     29        \u001b[36m0.6435\u001b[0m  0.1120\n",
            "     30        \u001b[36m0.6342\u001b[0m  0.1067\n",
            "     31        \u001b[36m0.6321\u001b[0m  0.1036\n",
            "     32        \u001b[36m0.6256\u001b[0m  0.0995\n",
            "     33        0.6274  0.1059\n",
            "     34        0.6287  0.1056\n",
            "     35        \u001b[36m0.6240\u001b[0m  0.0968\n",
            "     36        \u001b[36m0.6167\u001b[0m  0.1003\n",
            "     37        \u001b[36m0.6149\u001b[0m  0.0979\n",
            "     38        0.6206  0.1076\n",
            "     39        0.6201  0.1024\n",
            "     40        0.6190  0.1046\n",
            "     41        0.6173  0.1031\n",
            "     42        0.6171  0.0988\n",
            "     43        0.6180  0.1145\n",
            "     44        0.6189  0.1024\n",
            "     45        \u001b[36m0.6109\u001b[0m  0.1179\n",
            "     46        0.6166  0.1302\n",
            "     47        0.6163  0.1216\n",
            "     48        0.6129  0.1254\n",
            "     49        0.6162  0.1124\n",
            "     50        0.6131  0.1165\n",
            "     51        0.6193  0.1235\n",
            "     52        0.6138  0.1285\n",
            "     53        0.6137  0.1191\n",
            "     54        0.6137  0.1121\n",
            "     55        0.6158  0.1143\n",
            "     56        0.6148  0.0918\n",
            "     57        0.6171  0.0799\n",
            "     58        0.6177  0.0772\n",
            "     59        0.6124  0.0770\n",
            "     60        0.6167  0.0813\n",
            "     61        0.6166  0.0821\n",
            "     62        0.6125  0.0824\n",
            "     63        0.6133  0.0775\n",
            "     64        0.6122  0.0789\n",
            "     65        0.6145  0.0941\n",
            "     66        0.6122  0.1383\n",
            "     67        0.6163  0.1209\n",
            "     68        0.6110  0.1551\n",
            "     69        0.6153  0.1423\n",
            "     70        0.6165  0.1424\n",
            "     71        0.6133  0.0765\n",
            "     72        0.6141  0.0848\n",
            "     73        0.6162  0.0934\n",
            "     74        0.6131  0.1256\n",
            "     75        0.6172  0.1315\n",
            "     76        0.6162  0.1409\n",
            "     77        0.6170  0.1279\n",
            "     78        0.6193  0.1433\n",
            "     79        0.6130  0.1279\n",
            "     80        0.6154  0.1235\n",
            "     81        0.6160  0.1261\n",
            "     82        0.6129  0.1268\n",
            "     83        0.6143  0.1377\n",
            "     84        0.6140  0.1272\n",
            "     85        0.6140  0.1358\n",
            "     86        0.6132  0.1363\n",
            "     87        0.6117  0.1360\n",
            "     88        0.6131  0.1330\n",
            "     89        0.6148  0.1177\n",
            "     90        0.6160  0.1273\n",
            "     91        0.6137  0.1789\n",
            "     92        0.6155  0.3465\n",
            "     93        \u001b[36m0.6108\u001b[0m  0.2630\n",
            "     94        0.6203  0.1589\n",
            "     95        0.6121  0.0842\n",
            "     96        0.6147  0.0835\n",
            "     97        0.6150  0.0759\n",
            "     98        0.6162  0.0803\n",
            "     99        0.6142  0.0852\n",
            "    100        0.6115  0.0817\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0791\n",
            "      2       37.3047  0.0783\n",
            "      3       37.3047  0.0769\n",
            "      4       37.3047  0.0778\n",
            "      5       37.3047  0.0799\n",
            "      6       37.3047  0.0841\n",
            "      7       37.3047  0.0782\n",
            "      8       37.3047  0.0886\n",
            "      9       37.3047  0.0747\n",
            "     10       37.3047  0.0821\n",
            "     11       37.3047  0.0755\n",
            "     12       37.3047  0.0771\n",
            "     13       37.3047  0.0813\n",
            "     14       37.3047  0.0763\n",
            "     15       37.3047  0.0781\n",
            "     16       37.3047  0.0756\n",
            "     17       37.3047  0.0806\n",
            "     18       37.3047  0.0724\n",
            "     19       37.3047  0.0778\n",
            "     20       37.3047  0.0795\n",
            "     21       37.3047  0.0832\n",
            "     22       37.3047  0.0743\n",
            "     23       \u001b[36m16.9707\u001b[0m  0.0803\n",
            "     24        \u001b[36m0.5670\u001b[0m  0.0823\n",
            "     25        \u001b[36m0.5552\u001b[0m  0.0770\n",
            "     26        \u001b[36m0.5076\u001b[0m  0.0761\n",
            "     27        0.5469  0.0767\n",
            "     28        \u001b[36m0.4929\u001b[0m  0.0831\n",
            "     29        \u001b[36m0.4814\u001b[0m  0.0781\n",
            "     30        \u001b[36m0.4388\u001b[0m  0.0770\n",
            "     31        0.4476  0.0801\n",
            "     32        0.4458  0.0864\n",
            "     33        \u001b[36m0.4268\u001b[0m  0.0885\n",
            "     34        0.4278  0.0767\n",
            "     35        0.4288  0.0809\n",
            "     36        \u001b[36m0.4160\u001b[0m  0.0816\n",
            "     37        \u001b[36m0.4055\u001b[0m  0.0788\n",
            "     38        \u001b[36m0.3747\u001b[0m  0.0788\n",
            "     39        \u001b[36m0.3715\u001b[0m  0.0779\n",
            "     40        0.3873  0.0764\n",
            "     41        0.3935  0.0802\n",
            "     42        \u001b[36m0.3646\u001b[0m  0.0776\n",
            "     43        0.3894  0.0778\n",
            "     44        0.3850  0.0784\n",
            "     45        0.3698  0.0839\n",
            "     46        \u001b[36m0.3598\u001b[0m  0.0898\n",
            "     47        \u001b[36m0.3311\u001b[0m  0.0800\n",
            "     48        0.3366  0.0787\n",
            "     49        0.3434  0.0823\n",
            "     50        \u001b[36m0.3214\u001b[0m  0.0769\n",
            "     51        0.3570  0.0759\n",
            "     52        \u001b[36m0.3187\u001b[0m  0.0789\n",
            "     53        0.3570  0.0797\n",
            "     54        \u001b[36m0.2981\u001b[0m  0.0992\n",
            "     55        0.3044  0.1034\n",
            "     56        0.3055  0.1025\n",
            "     57        0.3107  0.1024\n",
            "     58        0.3015  0.1150\n",
            "     59        0.3383  0.1013\n",
            "     60        \u001b[36m0.2890\u001b[0m  0.1007\n",
            "     61        0.3180  0.1022\n",
            "     62        0.3029  0.1084\n",
            "     63        \u001b[36m0.2770\u001b[0m  0.0986\n",
            "     64        \u001b[36m0.2714\u001b[0m  0.1075\n",
            "     65        0.3085  0.1035\n",
            "     66        0.2882  0.1166\n",
            "     67        0.2743  0.1144\n",
            "     68        0.2837  0.1060\n",
            "     69        0.2789  0.1068\n",
            "     70        0.2780  0.1020\n",
            "     71        0.2783  0.1048\n",
            "     72        0.2780  0.1053\n",
            "     73        0.2893  0.1072\n",
            "     74        \u001b[36m0.2482\u001b[0m  0.0978\n",
            "     75        0.2776  0.0987\n",
            "     76        \u001b[36m0.2390\u001b[0m  0.1068\n",
            "     77        0.2510  0.1096\n",
            "     78        0.2413  0.1098\n",
            "     79        0.2669  0.1055\n",
            "     80        0.2672  0.1030\n",
            "     81        0.2736  0.1052\n",
            "     82        \u001b[36m0.2359\u001b[0m  0.1021\n",
            "     83        0.2539  0.1017\n",
            "     84        0.2692  0.0996\n",
            "     85        0.2767  0.1019\n",
            "     86        0.2412  0.1206\n",
            "     87        \u001b[36m0.2285\u001b[0m  0.1047\n",
            "     88        0.2639  0.1605\n",
            "     89        0.2397  0.1542\n",
            "     90        0.2581  0.1241\n",
            "     91        \u001b[36m0.2231\u001b[0m  0.2046\n",
            "     92        0.2284  0.2626\n",
            "     93        0.2252  0.2439\n",
            "     94        0.2647  0.1307\n",
            "     95        0.2320  0.1263\n",
            "     96        0.2433  0.1219\n",
            "     97        0.2352  0.1248\n",
            "     98        0.2690  0.1240\n",
            "     99        0.2713  0.1354\n",
            "    100        0.2456  0.1173\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.1212\n",
            "      2       37.3047  0.1333\n",
            "      3       37.3047  0.1244\n",
            "      4       37.3047  0.1191\n",
            "      5       37.3047  0.1218\n",
            "      6       37.3047  0.1124\n",
            "      7       37.3047  0.1389\n",
            "      8       37.3047  0.1070\n",
            "      9       37.3047  0.0782\n",
            "     10       37.3047  0.0773\n",
            "     11       37.3047  0.0750\n",
            "     12       37.3047  0.0763\n",
            "     13       37.3047  0.0878\n",
            "     14       37.3047  0.0867\n",
            "     15       37.3047  0.0819\n",
            "     16       37.3047  0.0795\n",
            "     17       37.3047  0.0804\n",
            "     18       37.3047  0.0830\n",
            "     19       37.3047  0.0801\n",
            "     20       37.3047  0.0777\n",
            "     21       37.3047  0.0722\n",
            "     22       37.3047  0.0741\n",
            "     23       \u001b[36m37.1336\u001b[0m  0.0736\n",
            "     24       37.1392  0.0839\n",
            "     25       \u001b[36m16.6642\u001b[0m  0.0760\n",
            "     26        \u001b[36m0.6378\u001b[0m  0.0761\n",
            "     27        \u001b[36m0.6137\u001b[0m  0.0731\n",
            "     28        \u001b[36m0.6018\u001b[0m  0.0818\n",
            "     29        \u001b[36m0.5881\u001b[0m  0.0747\n",
            "     30        \u001b[36m0.5694\u001b[0m  0.0741\n",
            "     31        0.5814  0.0787\n",
            "     32        \u001b[36m0.5663\u001b[0m  0.0800\n",
            "     33        \u001b[36m0.5500\u001b[0m  0.0751\n",
            "     34        \u001b[36m0.5411\u001b[0m  0.0779\n",
            "     35        \u001b[36m0.5408\u001b[0m  0.0857\n",
            "     36        \u001b[36m0.5198\u001b[0m  0.0786\n",
            "     37        \u001b[36m0.5055\u001b[0m  0.0818\n",
            "     38        0.5086  0.0744\n",
            "     39        \u001b[36m0.4620\u001b[0m  0.0741\n",
            "     40        0.4657  0.0737\n",
            "     41        0.4768  0.0805\n",
            "     42        \u001b[36m0.4422\u001b[0m  0.0785\n",
            "     43        \u001b[36m0.4326\u001b[0m  0.1120\n",
            "     44        \u001b[36m0.4229\u001b[0m  0.1304\n",
            "     45        \u001b[36m0.4092\u001b[0m  0.1157\n",
            "     46        \u001b[36m0.3827\u001b[0m  0.1124\n",
            "     47        \u001b[36m0.3629\u001b[0m  0.1204\n",
            "     48        \u001b[36m0.3431\u001b[0m  0.1307\n",
            "     49        0.3462  0.1219\n",
            "     50        0.5408  0.1405\n",
            "     51        0.3542  0.2117\n",
            "     52        0.3605  0.2053\n",
            "     53        0.3783  0.1824\n",
            "     54        \u001b[36m0.3056\u001b[0m  0.2349\n",
            "     55        0.3301  0.1126\n",
            "     56        0.3146  0.0763\n",
            "     57        \u001b[36m0.3026\u001b[0m  0.0922\n",
            "     58        \u001b[36m0.2810\u001b[0m  0.0925\n",
            "     59        0.2955  0.0751\n",
            "     60        0.2882  0.0837\n",
            "     61        \u001b[36m0.2748\u001b[0m  0.0760\n",
            "     62        0.2783  0.0823\n",
            "     63        \u001b[36m0.2463\u001b[0m  0.0780\n",
            "     64        0.2996  0.0838\n",
            "     65        0.2795  0.0783\n",
            "     66        0.2777  0.0818\n",
            "     67        0.2528  0.0789\n",
            "     68        0.2775  0.0756\n",
            "     69        0.2848  0.0772\n",
            "     70        0.2484  0.0811\n",
            "     71        0.3686  0.0837\n",
            "     72        0.2502  0.0756\n",
            "     73        0.2611  0.0740\n",
            "     74        0.2714  0.0813\n",
            "     75        \u001b[36m0.2300\u001b[0m  0.0765\n",
            "     76        0.2612  0.0774\n",
            "     77        0.2555  0.0776\n",
            "     78        0.2632  0.0780\n",
            "     79        0.2527  0.0812\n",
            "     80        0.2492  0.0793\n",
            "     81        0.2414  0.0766\n",
            "     82        0.2568  0.0750\n",
            "     83        0.2438  0.0782\n",
            "     84        0.2605  0.0837\n",
            "     85        0.2365  0.0887\n",
            "     86        0.2385  0.0784\n",
            "     87        0.2561  0.0771\n",
            "     88        0.2478  0.0734\n",
            "     89        \u001b[36m0.2241\u001b[0m  0.0763\n",
            "     90        0.2355  0.0815\n",
            "     91        0.2291  0.0749\n",
            "     92        0.2937  0.0766\n",
            "     93        0.2300  0.0788\n",
            "     94        0.2242  0.0791\n",
            "     95        0.2515  0.0807\n",
            "     96        \u001b[36m0.2216\u001b[0m  0.0863\n",
            "     97        0.2641  0.0977\n",
            "     98        0.2342  0.1097\n",
            "     99        0.2505  0.1056\n",
            "    100        0.2491  0.1024\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.1022\n",
            "      2       37.3047  0.1168\n",
            "      3       37.3047  0.1011\n",
            "      4       37.3047  0.1086\n",
            "      5       37.3047  0.1262\n",
            "      6       37.3047  0.1025\n",
            "      7       37.3047  0.2223\n",
            "      8       37.3047  0.2386\n",
            "      9       37.3047  0.2167\n",
            "     10       37.3047  0.2004\n",
            "     11       37.3047  0.1220\n",
            "     12       37.3047  0.1131\n",
            "     13       37.3047  0.1096\n",
            "     14       37.3047  0.2184\n",
            "     15       37.3047  0.1810\n",
            "     16       37.3047  0.2174\n",
            "     17       37.3047  0.2127\n",
            "     18       37.3047  0.2053\n",
            "     19       37.3047  0.2113\n",
            "     20       37.3047  0.2848\n",
            "     21       37.3047  0.2496\n",
            "     22       37.3047  0.2474\n",
            "     23       37.3047  0.2579\n",
            "     24       \u001b[36m24.7075\u001b[0m  0.1950\n",
            "     25        \u001b[36m0.6087\u001b[0m  0.1287\n",
            "     26        \u001b[36m0.5459\u001b[0m  0.1336\n",
            "     27        \u001b[36m0.5355\u001b[0m  0.1233\n",
            "     28        \u001b[36m0.5077\u001b[0m  0.1207\n",
            "     29        0.5294  0.1330\n",
            "     30        \u001b[36m0.4971\u001b[0m  0.1180\n",
            "     31        0.5012  0.0803\n",
            "     32        \u001b[36m0.4722\u001b[0m  0.0811\n",
            "     33        0.4785  0.0845\n",
            "     34        \u001b[36m0.4616\u001b[0m  0.0793\n",
            "     35        \u001b[36m0.4349\u001b[0m  0.0829\n",
            "     36        \u001b[36m0.4114\u001b[0m  0.0813\n",
            "     37        \u001b[36m0.4007\u001b[0m  0.0934\n",
            "     38        \u001b[36m0.3979\u001b[0m  0.0792\n",
            "     39        0.4114  0.0785\n",
            "     40        0.3990  0.0816\n",
            "     41        \u001b[36m0.3873\u001b[0m  0.0755\n",
            "     42        \u001b[36m0.3540\u001b[0m  0.0799\n",
            "     43        0.3756  0.0866\n",
            "     44        0.3581  0.0780\n",
            "     45        \u001b[36m0.3245\u001b[0m  0.0772\n",
            "     46        \u001b[36m0.3050\u001b[0m  0.0831\n",
            "     47        0.3411  0.0769\n",
            "     48        0.3053  0.0863\n",
            "     49        \u001b[36m0.2882\u001b[0m  0.0889\n",
            "     50        \u001b[36m0.2671\u001b[0m  0.0808\n",
            "     51        0.2799  0.0758\n",
            "     52        0.3579  0.0828\n",
            "     53        0.2879  0.0755\n",
            "     54        0.2721  0.0840\n",
            "     55        0.2719  0.0774\n",
            "     56        0.2742  0.0764\n",
            "     57        0.2717  0.0763\n",
            "     58        0.2815  0.0803\n",
            "     59        0.2738  0.0790\n",
            "     60        0.2763  0.0824\n",
            "     61        \u001b[36m0.2409\u001b[0m  0.0817\n",
            "     62        0.2498  0.0851\n",
            "     63        0.2439  0.0752\n",
            "     64        0.2556  0.0852\n",
            "     65        0.2426  0.0797\n",
            "     66        \u001b[36m0.2331\u001b[0m  0.0821\n",
            "     67        0.2446  0.0795\n",
            "     68        \u001b[36m0.2194\u001b[0m  0.0803\n",
            "     69        \u001b[36m0.2106\u001b[0m  0.0765\n",
            "     70        \u001b[36m0.2050\u001b[0m  0.0790\n",
            "     71        0.2249  0.0793\n",
            "     72        0.2662  0.0878\n",
            "     73        0.2384  0.0818\n",
            "     74        0.2430  0.0903\n",
            "     75        0.2759  0.0786\n",
            "     76        0.2362  0.0795\n",
            "     77        0.2385  0.0775\n",
            "     78        0.2309  0.0898\n",
            "     79        0.2500  0.0839\n",
            "     80        0.2328  0.0791\n",
            "     81        0.2144  0.0786\n",
            "     82        0.2573  0.0776\n",
            "     83        0.2334  0.0821\n",
            "     84        0.2429  0.0835\n",
            "     85        0.2967  0.0792\n",
            "     86        0.2405  0.0929\n",
            "     87        \u001b[36m0.2011\u001b[0m  0.0770\n",
            "     88        0.2358  0.0836\n",
            "     89        0.2187  0.0796\n",
            "     90        0.2097  0.0835\n",
            "     91        0.2016  0.0765\n",
            "     92        0.2230  0.0889\n",
            "     93        0.2051  0.0814\n",
            "     94        0.2128  0.0810\n",
            "     95        0.2210  0.0795\n",
            "     96        0.2162  0.0817\n",
            "     97        0.2209  0.0810\n",
            "     98        0.2163  0.0795\n",
            "     99        0.2162  0.0838\n",
            "    100        0.2117  0.0839\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0763\n",
            "      2       37.3047  0.0805\n",
            "      3       37.3047  0.0789\n",
            "      4       37.3047  0.0841\n",
            "      5       37.3047  0.0775\n",
            "      6       37.3047  0.0797\n",
            "      7       37.3047  0.0803\n",
            "      8       37.3047  0.0858\n",
            "      9       37.3047  0.0775\n",
            "     10       37.3047  0.0751\n",
            "     11       37.3047  0.0868\n",
            "     12       37.3047  0.0808\n",
            "     13       37.3047  0.0763\n",
            "     14       37.3047  0.0815\n",
            "     15       37.3047  0.0767\n",
            "     16       37.3047  0.0777\n",
            "     17       37.3047  0.1162\n",
            "     18       37.3047  0.1293\n",
            "     19       37.3047  0.0972\n",
            "     20       37.3047  0.0820\n",
            "     21       37.3047  0.0881\n",
            "     22       \u001b[36m36.9849\u001b[0m  0.1468\n",
            "     23       \u001b[36m11.4798\u001b[0m  0.1272\n",
            "     24        \u001b[36m0.5718\u001b[0m  0.1262\n",
            "     25        \u001b[36m0.5398\u001b[0m  0.1370\n",
            "     26        0.5609  0.1310\n",
            "     27        \u001b[36m0.5084\u001b[0m  0.1270\n",
            "     28        0.5136  0.1281\n",
            "     29        \u001b[36m0.4879\u001b[0m  0.1167\n",
            "     30        0.4934  0.1249\n",
            "     31        \u001b[36m0.4767\u001b[0m  0.1287\n",
            "     32        0.4783  0.2458\n",
            "     33        \u001b[36m0.4497\u001b[0m  0.3076\n",
            "     34        0.4657  0.3358\n",
            "     35        0.4639  0.2556\n",
            "     36        0.4742  0.2863\n",
            "     37        \u001b[36m0.4376\u001b[0m  0.3928\n",
            "     38        \u001b[36m0.4129\u001b[0m  0.3268\n",
            "     39        0.4285  0.2828\n",
            "     40        0.4327  0.2100\n",
            "     41        0.4208  0.2292\n",
            "     42        0.4182  0.2211\n",
            "     43        \u001b[36m0.3966\u001b[0m  0.1703\n",
            "     44        0.4234  0.0993\n",
            "     45        \u001b[36m0.3818\u001b[0m  0.1043\n",
            "     46        0.4080  0.1093\n",
            "     47        0.4039  0.0994\n",
            "     48        0.3967  0.1003\n",
            "     49        \u001b[36m0.3726\u001b[0m  0.1204\n",
            "     50        \u001b[36m0.3653\u001b[0m  0.1091\n",
            "     51        \u001b[36m0.3502\u001b[0m  0.1074\n",
            "     52        0.3783  0.1054\n",
            "     53        \u001b[36m0.3458\u001b[0m  0.1189\n",
            "     54        0.4075  0.1238\n",
            "     55        0.3694  0.1265\n",
            "     56        0.3671  0.1229\n",
            "     57        0.3465  0.1212\n",
            "     58        \u001b[36m0.3401\u001b[0m  0.1196\n",
            "     59        0.3445  0.1165\n",
            "     60        0.3479  0.1037\n",
            "     61        0.3429  0.0771\n",
            "     62        \u001b[36m0.3211\u001b[0m  0.0766\n",
            "     63        0.3270  0.0761\n",
            "     64        0.3443  0.0819\n",
            "     65        0.3490  0.0829\n",
            "     66        0.3483  0.0816\n",
            "     67        \u001b[36m0.3134\u001b[0m  0.0794\n",
            "     68        0.3281  0.0814\n",
            "     69        0.3343  0.0921\n",
            "     70        0.3184  0.0800\n",
            "     71        0.3499  0.0792\n",
            "     72        0.3457  0.0784\n",
            "     73        \u001b[36m0.3071\u001b[0m  0.0800\n",
            "     74        \u001b[36m0.3027\u001b[0m  0.0776\n",
            "     75        0.3138  0.0745\n",
            "     76        0.3325  0.0767\n",
            "     77        0.3307  0.0784\n",
            "     78        \u001b[36m0.2881\u001b[0m  0.0759\n",
            "     79        0.3379  0.0776\n",
            "     80        0.3562  0.0829\n",
            "     81        0.3168  0.0914\n",
            "     82        0.2992  0.0808\n",
            "     83        0.3209  0.0779\n",
            "     84        0.3050  0.0788\n",
            "     85        0.3216  0.0799\n",
            "     86        0.3074  0.0820\n",
            "     87        0.2948  0.0794\n",
            "     88        0.3217  0.0815\n",
            "     89        0.3219  0.0807\n",
            "     90        0.3285  0.0763\n",
            "     91        0.3178  0.0803\n",
            "     92        0.3349  0.0821\n",
            "     93        0.3128  0.0916\n",
            "     94        0.3036  0.1073\n",
            "     95        0.3052  0.0761\n",
            "     96        0.3475  0.0782\n",
            "     97        0.3312  0.0809\n",
            "     98        0.3083  0.0769\n",
            "     99        0.3058  0.0772\n",
            "    100        0.2941  0.0802\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0753\n",
            "      2       37.3047  0.0759\n",
            "      3       37.3047  0.0773\n",
            "      4       37.3047  0.0784\n",
            "      5       37.3047  0.0808\n",
            "      6       37.3047  0.0871\n",
            "      7       37.3047  0.0799\n",
            "      8       37.3047  0.0784\n",
            "      9       37.3047  0.0829\n",
            "     10       37.3047  0.0761\n",
            "     11       37.3047  0.0846\n",
            "     12       37.3047  0.0843\n",
            "     13       37.3047  0.0799\n",
            "     14       37.3047  0.0781\n",
            "     15       37.3047  0.0789\n",
            "     16       37.3047  0.0761\n",
            "     17       37.3047  0.0839\n",
            "     18       37.3047  0.0809\n",
            "     19       37.3047  0.0836\n",
            "     20       37.3047  0.0761\n",
            "     21       37.3047  0.0787\n",
            "     22       37.3047  0.0835\n",
            "     23       \u001b[36m21.4992\u001b[0m  0.0809\n",
            "     24        \u001b[36m0.5613\u001b[0m  0.0808\n",
            "     25        0.6100  0.0816\n",
            "     26        \u001b[36m0.5529\u001b[0m  0.0789\n",
            "     27        \u001b[36m0.5462\u001b[0m  0.0819\n",
            "     28        \u001b[36m0.5204\u001b[0m  0.0756\n",
            "     29        0.5234  0.0806\n",
            "     30        \u001b[36m0.5193\u001b[0m  0.0800\n",
            "     31        0.5241  0.0869\n",
            "     32        \u001b[36m0.5048\u001b[0m  0.0740\n",
            "     33        \u001b[36m0.4852\u001b[0m  0.0811\n",
            "     34        0.4962  0.0806\n",
            "     35        \u001b[36m0.4808\u001b[0m  0.0784\n",
            "     36        \u001b[36m0.4681\u001b[0m  0.0790\n",
            "     37        \u001b[36m0.4509\u001b[0m  0.0849\n",
            "     38        0.4611  0.0798\n",
            "     39        \u001b[36m0.4451\u001b[0m  0.0850\n",
            "     40        \u001b[36m0.4050\u001b[0m  0.0779\n",
            "     41        0.4152  0.0786\n",
            "     42        \u001b[36m0.4025\u001b[0m  0.0751\n",
            "     43        \u001b[36m0.3929\u001b[0m  0.1016\n",
            "     44        0.4487  0.0749\n",
            "     45        \u001b[36m0.3711\u001b[0m  0.0788\n",
            "     46        \u001b[36m0.3596\u001b[0m  0.0816\n",
            "     47        \u001b[36m0.3593\u001b[0m  0.0890\n",
            "     48        \u001b[36m0.3467\u001b[0m  0.0803\n",
            "     49        0.3570  0.0782\n",
            "     50        0.3675  0.0821\n",
            "     51        \u001b[36m0.3448\u001b[0m  0.0791\n",
            "     52        \u001b[36m0.3195\u001b[0m  0.0814\n",
            "     53        \u001b[36m0.3133\u001b[0m  0.0777\n",
            "     54        0.3308  0.0863\n",
            "     55        0.3290  0.0801\n",
            "     56        \u001b[36m0.2996\u001b[0m  0.0961\n",
            "     57        0.3492  0.0777\n",
            "     58        0.3076  0.0829\n",
            "     59        0.3019  0.0775\n",
            "     60        \u001b[36m0.2605\u001b[0m  0.0735\n",
            "     61        0.2992  0.0761\n",
            "     62        0.2866  0.0775\n",
            "     63        0.3054  0.0776\n",
            "     64        0.3170  0.0815\n",
            "     65        0.2738  0.0768\n",
            "     66        0.2867  0.0786\n",
            "     67        0.2887  0.0781\n",
            "     68        0.2821  0.0920\n",
            "     69        0.2651  0.0856\n",
            "     70        \u001b[36m0.2554\u001b[0m  0.0812\n",
            "     71        0.2624  0.0781\n",
            "     72        0.2617  0.0847\n",
            "     73        \u001b[36m0.2502\u001b[0m  0.0752\n",
            "     74        0.2609  0.0762\n",
            "     75        0.2575  0.0752\n",
            "     76        0.2515  0.0839\n",
            "     77        0.2599  0.0827\n",
            "     78        0.2673  0.0785\n",
            "     79        0.2504  0.0798\n",
            "     80        \u001b[36m0.2404\u001b[0m  0.0854\n",
            "     81        0.2592  0.0874\n",
            "     82        0.2696  0.1125\n",
            "     83        \u001b[36m0.2181\u001b[0m  0.1092\n",
            "     84        0.2469  0.1153\n",
            "     85        0.2500  0.0988\n",
            "     86        0.2350  0.1012\n",
            "     87        0.2572  0.0997\n",
            "     88        0.2495  0.0988\n",
            "     89        0.2432  0.1005\n",
            "     90        0.2255  0.1137\n",
            "     91        0.2295  0.1176\n",
            "     92        0.2420  0.1009\n",
            "     93        0.2457  0.0977\n",
            "     94        0.2374  0.1054\n",
            "     95        0.2406  0.1001\n",
            "     96        0.2360  0.1096\n",
            "     97        0.2340  0.1083\n",
            "     98        0.2252  0.1021\n",
            "     99        0.2237  0.0994\n",
            "    100        0.2752  0.1064\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.2320\u001b[0m  0.0992\n",
            "      2       37.2320  0.0949\n",
            "      3       37.2320  0.1031\n",
            "      4       37.2320  0.1071\n",
            "      5       37.2320  0.1023\n",
            "      6       37.2320  0.1016\n",
            "      7       37.2320  0.1014\n",
            "      8       37.2320  0.1009\n",
            "      9       37.2320  0.1031\n",
            "     10       37.2320  0.1023\n",
            "     11       37.2320  0.1092\n",
            "     12       37.2320  0.1016\n",
            "     13       37.2320  0.1076\n",
            "     14       37.2320  0.1036\n",
            "     15       37.2320  0.1066\n",
            "     16       37.2320  0.1096\n",
            "     17       37.2320  0.1172\n",
            "     18       37.2320  0.1138\n",
            "     19       37.2320  0.1190\n",
            "     20       37.2320  0.1159\n",
            "     21       37.2320  0.1211\n",
            "     22       \u001b[36m36.7278\u001b[0m  0.1331\n",
            "     23       \u001b[36m17.4015\u001b[0m  0.1206\n",
            "     24        \u001b[36m0.6029\u001b[0m  0.0852\n",
            "     25        \u001b[36m0.5757\u001b[0m  0.0863\n",
            "     26        \u001b[36m0.5581\u001b[0m  0.0787\n",
            "     27        0.5730  0.0883\n",
            "     28        \u001b[36m0.5338\u001b[0m  0.0860\n",
            "     29        \u001b[36m0.5321\u001b[0m  0.0863\n",
            "     30        \u001b[36m0.5305\u001b[0m  0.0795\n",
            "     31        \u001b[36m0.5133\u001b[0m  0.0864\n",
            "     32        \u001b[36m0.5076\u001b[0m  0.0814\n",
            "     33        0.5184  0.0792\n",
            "     34        0.5094  0.0781\n",
            "     35        \u001b[36m0.4833\u001b[0m  0.0857\n",
            "     36        0.5106  0.0811\n",
            "     37        \u001b[36m0.4814\u001b[0m  0.0822\n",
            "     38        0.4836  0.0784\n",
            "     39        0.4887  0.0769\n",
            "     40        \u001b[36m0.4801\u001b[0m  0.0805\n",
            "     41        \u001b[36m0.4441\u001b[0m  0.0851\n",
            "     42        0.4724  0.0851\n",
            "     43        0.4555  0.0787\n",
            "     44        \u001b[36m0.4438\u001b[0m  0.0795\n",
            "     45        \u001b[36m0.4383\u001b[0m  0.0778\n",
            "     46        0.4853  0.0729\n",
            "     47        \u001b[36m0.4126\u001b[0m  0.0770\n",
            "     48        0.4156  0.0803\n",
            "     49        \u001b[36m0.3894\u001b[0m  0.0757\n",
            "     50        \u001b[36m0.3849\u001b[0m  0.0742\n",
            "     51        0.3866  0.0779\n",
            "     52        \u001b[36m0.3632\u001b[0m  0.0710\n",
            "     53        \u001b[36m0.3515\u001b[0m  0.0750\n",
            "     54        \u001b[36m0.3338\u001b[0m  0.0892\n",
            "     55        0.3511  0.0735\n",
            "     56        \u001b[36m0.3320\u001b[0m  0.0765\n",
            "     57        \u001b[36m0.3141\u001b[0m  0.0757\n",
            "     58        \u001b[36m0.3092\u001b[0m  0.0815\n",
            "     59        0.3118  0.0748\n",
            "     60        0.3254  0.0756\n",
            "     61        \u001b[36m0.2997\u001b[0m  0.0768\n",
            "     62        \u001b[36m0.2807\u001b[0m  0.0834\n",
            "     63        0.2982  0.0768\n",
            "     64        0.3084  0.0811\n",
            "     65        0.2816  0.0861\n",
            "     66        \u001b[36m0.2749\u001b[0m  0.0836\n",
            "     67        \u001b[36m0.2660\u001b[0m  0.0822\n",
            "     68        \u001b[36m0.2533\u001b[0m  0.0888\n",
            "     69        0.2576  0.0778\n",
            "     70        0.2592  0.0779\n",
            "     71        0.2647  0.0787\n",
            "     72        \u001b[36m0.2297\u001b[0m  0.0768\n",
            "     73        0.2476  0.0732\n",
            "     74        \u001b[36m0.2168\u001b[0m  0.0739\n",
            "     75        \u001b[36m0.2141\u001b[0m  0.0767\n",
            "     76        0.2232  0.0815\n",
            "     77        0.2411  0.0769\n",
            "     78        0.2498  0.0791\n",
            "     79        0.2438  0.0907\n",
            "     80        0.2270  0.0819\n",
            "     81        0.2464  0.0762\n",
            "     82        \u001b[36m0.2022\u001b[0m  0.0766\n",
            "     83        0.2284  0.0742\n",
            "     84        0.2286  0.0783\n",
            "     85        0.2388  0.0862\n",
            "     86        0.3194  0.0792\n",
            "     87        0.2179  0.0791\n",
            "     88        0.2154  0.0792\n",
            "     89        0.2126  0.0833\n",
            "     90        0.2217  0.0811\n",
            "     91        0.2197  0.0822\n",
            "     92        0.2194  0.0847\n",
            "     93        0.2137  0.0781\n",
            "     94        0.2035  0.0793\n",
            "     95        0.2034  0.0780\n",
            "     96        0.2826  0.0785\n",
            "     97        0.2042  0.0794\n",
            "     98        0.2060  0.0810\n",
            "     99        0.2317  0.0804\n",
            "    100        0.2145  0.0793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-yIZ5IFIdx2",
        "outputId": "7ccd4952-9561-4d34-b012-0f4113ca7740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-N2cqUoIg9J",
        "outputId": "cae9b165-68f5-444e-b169-234f4b145189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84210526, 0.63157895, 0.87719298, 0.63157895, 0.89473684,\n",
              "       0.89473684, 0.84210526, 0.94736842, 0.87719298, 0.875     ])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média\n",
        "media = resultados.mean()\n",
        "media"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXXWA_zAIj-o",
        "outputId": "28ade336-2477-4988-aa86-6e3aad8c2c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8313596491228068"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desvio Padrão\n",
        "desvio = resultados.std()\n",
        "desvio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYw2B6txIm08",
        "outputId": "3db57037-86bb-4c0e-84e3-3c4602c64cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10375588440558078"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}